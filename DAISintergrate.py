#use myself code
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import tensorflow as tf
import numpy as np
import argparse
import os
import json
import glob
import random
import collections
import math
import time
import cv2
# import historgramloss
# åƒè€ƒ

# https://colab.research.google.com/drive/182CGDnFxt08NmjCCTu5jDweUjn3jhB2y
parser = argparse.ArgumentParser()
#--input_dir:åŒ…å«åœ–åƒçš„æ–‡ä»¶å¤¾è·¯å¾‘ã€‚
parser.add_argument("--input_dir", help="path to folder containing images")
# which_direction #é¸é …ï¼štrain, test, export é‹è¡Œæ¨¡å¼ã€‚
parser.add_argument("--mode", choices=["train", "test", "export"])
# è¼¸å‡ºæ–‡ä»¶å­˜æ”¾ä½ç½®ã€‚
parser.add_argument("--output_dir", help="where to put output files")
#--seed:é¡å‹ï¼šint èªªæ˜ï¼šéš¨æ©Ÿç¨®å­
parser.add_argument("--seed", type=int)
#--checkpoint:èªªæ˜ï¼šè¦æ¢å¾©è¨“ç·´æˆ–æ¸¬è©¦çš„æª¢æŸ¥é»ç›®éŒ„ã€‚ ç”¨é€”ï¼šæŒ‡å®šæå–ç‰¹å¾µçš„æª¢æŸ¥é»ç›®éŒ„ã€‚
parser.add_argument("--checkpoint", default=None,
                    help="directory with checkpoint to resume training from or use for testing")
#--cktCentralSul:èªªæ˜ï¼šæå–ç‰™é½’ä¸­å¤®æºç‰¹å¾µçš„æª¢æŸ¥é»ç›®éŒ„ã€‚
parser.add_argument("--cktCentralSul", default=None,
                    help="directory with checkpoint to extract teeth central groove features")
#--max_steps:èªªæ˜ï¼šè¨“ç·´æ­¥æ•¸ï¼ˆè¨­ç‚º0å‰‡ç¦ç”¨ï¼‰ã€‚ç”¨é€”ï¼šé™åˆ¶è¨“ç·´çš„æœ€å¤§æ­¥æ•¸ã€‚
parser.add_argument("--max_steps", type=int, help="number of training steps (0 to disable)")
#--max_epochs:é¡å‹ï¼šintèªªæ˜ï¼šè¨“ç·´è¼ªæ•¸ã€‚ç”¨é€”ï¼šé™åˆ¶è¨“ç·´çš„æœ€å¤§è¼ªæ•¸ã€‚
parser.add_argument("--max_epochs", type=int, help="number of training epochs")
#--summary_freq:é¡å‹ï¼šinté»˜èªå€¼ï¼š100èªªæ˜ï¼šæ›´æ–°å’Œä¿å­˜è¨“ç·´éç¨‹çš„æ‘˜è¦ä¿¡æ¯ã€‚åŒ…æ‹¬æå¤±å‡½æ•¸å€¼ã€å­¸ç¿’ç‡ã€æ¨¡å‹åƒæ•¸åˆ†ä½ˆç­‰
parser.add_argument("--summary_freq", type=int, default=100, help="update summaries every summary_freq steps")
#--progress_freq:é¡å‹ï¼šinté»˜èªå€¼ï¼š50èªªæ˜ï¼šç”¨é€”ï¼šåŒ…æ‹¬ç•¶å‰æ­¥æ•¸ã€æå¤±å€¼ã€è¨“ç·´é€Ÿåº¦ç­‰å³æ™‚ä¿¡æ¯ã€‚
parser.add_argument("--progress_freq", type=int, default=50, help="display progress every progress_freq steps")
#--trace_freq:é¡å‹ï¼šinté»˜èªå€¼ï¼š0èªªæ˜ï¼šåŒ…æ‹¬æ¯å€‹æ“ä½œçš„åŸ·è¡Œæ™‚é–“ã€å…§å­˜ä½¿ç”¨ç­‰ã€‚è·Ÿè¹¤æœƒé¡¯è‘—é™ä½åŸ·è¡Œé€Ÿåº¦ï¼Œæ‰€ä»¥é»˜èªå€¼ç‚º0ï¼ˆå³ä¸è·Ÿè¹¤ï¼‰ã€‚
parser.add_argument("--trace_freq", type=int, default=0, help="trace execution every trace_freq steps")
#--display_freq:é¡å‹ï¼šinté»˜èªå€¼ï¼š2000èªªæ˜ï¼šæ¯display_freqæ­¥å¯«ç•¶å‰è¨“ç·´åœ–åƒã€‚ç”¨é€”ï¼šè¨­ç½®åœ–åƒé¡¯ç¤ºçš„é »ç‡ã€‚
parser.add_argument("--display_freq", type=int, default=5000,
                    help="write current training images every display_freq steps")
# --save_freq:é¡å‹ï¼šinté»˜èªå€¼ï¼š2000èªªæ˜ï¼šæ¯save_freqæ­¥ä¿å­˜æ¨¡å‹ï¼ˆè¨­ç‚º0å‰‡ç¦ç”¨ï¼‰ã€‚ç”¨é€”ï¼šè¨­ç½®æ¨¡å‹ä¿å­˜çš„é »ç‡ã€‚
parser.add_argument("--save_freq", type=int, default=5000, help="save model every save_freq steps, 0 to disable")
#--aspect_ratio:é¡å‹ï¼šfloaté»˜èªå€¼ï¼š1.0èªªæ˜ï¼šè¼¸å‡ºåœ–åƒçš„å¯¬é«˜æ¯”ã€‚ç”¨é€”ï¼šè¨­ç½®è¼¸å‡ºåœ–åƒçš„å¯¬é«˜æ¯”ã€‚
parser.add_argument("--aspect_ratio", type=float, default=1.0, help="aspect ratio of output images (width/height)")
#--lab_colorization:é¡å‹ï¼šå¸ƒçˆ¾èªªæ˜ï¼šå°‡è¼¸å…¥åœ–åƒåˆ†ç‚ºäº®åº¦ï¼ˆAï¼‰å’Œé¡è‰²ï¼ˆBï¼‰ã€‚ç”¨é€”ï¼šå•Ÿç”¨æˆ–ç¦ç”¨LABé¡è‰²åˆ†é›¢ã€‚
parser.add_argument("--lab_colorization", action="store_true",
                    help="split input image into brightness (A) and color (B)")
#--batch_size:é¡å‹ï¼šinté»˜èªå€¼ï¼š1èªªæ˜ï¼šæ‰¹æ¬¡ä¸­çš„åœ–åƒæ•¸é‡ã€‚ç”¨é€”ï¼šè¨­ç½®è¨“ç·´æ‰¹æ¬¡çš„å¤§å°ã€‚
parser.add_argument("--batch_size", type=int, default=1, help="number of images in batch")
# --which_direction:é¡å‹ï¼šstré»˜èªå€¼ï¼šAtoBé¸é …ï¼šAtoB, BtoAèªªæ˜ï¼šåœ–åƒè½‰æ›æ–¹å‘ã€‚ç”¨é€”ï¼šæŒ‡å®šåœ–åƒè½‰æ›çš„æ–¹å‘ã€‚
parser.add_argument("--which_direction", type=str, default="AtoB", choices=["AtoB", "BtoA"])
# --ngf:é¡å‹ï¼šinté»˜èªå€¼ï¼š64èªªæ˜ï¼šç¬¬ä¸€å€‹å·ç©å±¤ä¸­ç”Ÿæˆå™¨æ¿¾æ³¢å™¨çš„æ•¸é‡ã€‚ç”¨é€”ï¼šè¨­ç½®ç”Ÿæˆå™¨ç¬¬ä¸€å±¤çš„æ¿¾æ³¢å™¨æ•¸é‡ã€‚
parser.add_argument("--ngf", type=int, default=64, help="number of generator filters in first conv layer")
# --ndf:é¡å‹ï¼šinté»˜èªå€¼ï¼š64èªªæ˜ï¼šç¬¬ä¸€å€‹å·ç©å±¤ä¸­åˆ¤åˆ¥å™¨æ¿¾æ³¢å™¨çš„æ•¸é‡ã€‚ç”¨é€”ï¼šè¨­ç½®åˆ¤åˆ¥å™¨ç¬¬ä¸€å±¤çš„æ¿¾æ³¢å™¨æ•¸é‡ã€‚
parser.add_argument("--ndf", type=int, default=64, help="number of discriminator filters in first conv layer")
# --nldf:é¡å‹ï¼šinté»˜èªå€¼ï¼š128èªªæ˜ï¼šç¬¬ä¸€å€‹å·ç©å±¤ä¸­å±€éƒ¨åˆ¤åˆ¥å™¨æ¿¾æ³¢å™¨çš„æ•¸é‡ã€‚ç”¨é€”ï¼šè¨­ç½®å±€éƒ¨åˆ¤åˆ¥å™¨ç¬¬ä¸€å±¤çš„æ¿¾æ³¢å™¨æ•¸é‡ã€‚
parser.add_argument("--nldf", type=int, default=128, help="number of local discriminator filters in first conv layer")
parser.add_argument("--scale_size", type=int, default=256, help="scale images to this size before cropping to 256x256")
#--scale_size:é¡å‹ï¼šinté»˜èªå€¼ï¼š800èªªæ˜ï¼šåœ¨è£å‰ªåˆ°256x256ä¹‹å‰å°‡åœ–åƒç¸®æ”¾åˆ°æ­¤å¤§å°ã€‚ç”¨é€”ï¼šè¨­ç½®åœ–åƒç¸®æ”¾çš„å¤§å°ã€‚
parser.add_argument("--flip", dest="flip", action="store_true", help="flip images horizontally")
parser.add_argument("--no_flip", dest="flip", action="store_false", help="don't flip images horizontally")
# --flip å’Œ --no_flip:é¡å‹ï¼šå¸ƒçˆ¾èªªæ˜ï¼šæ°´å¹³ç¿»è½‰åœ–åƒã€‚ç”¨é€”ï¼šè¨­ç½®æ˜¯å¦æ°´å¹³ç¿»è½‰åœ–åƒã€‚
parser.set_defaults(flip=True)
parser.add_argument("--lr", type=float, default=0.0002, help="initial learning rate for adam")
#--lr:é¡å‹ï¼šfloaté»˜èªå€¼ï¼š0.0002èªªæ˜ï¼šAdamå„ªåŒ–å™¨çš„åˆå§‹å­¸ç¿’ç‡ã€‚ç”¨é€”ï¼šè¨­ç½®å­¸ç¿’ç‡ã€‚
parser.add_argument("--beta1", type=float, default=0.5, help="momentum term of adam")
# --beta1:é¡å‹ï¼šfloaté»˜èªå€¼ï¼š0.5èªªæ˜ï¼šAdamå„ªåŒ–å™¨çš„å‹•é‡é …ã€‚ç”¨é€”ï¼šè¨­ç½®Adamå„ªåŒ–å™¨çš„å‹•é‡ã€‚
parser.add_argument("--l1_weight", type=float, default=100.0, help="weight on L1 term for generator gradient")
#--l1_weight:é¡å‹ï¼šfloaté»˜èªå€¼ï¼š100.0èªªæ˜ï¼šç”Ÿæˆå™¨æ¢¯åº¦çš„L1é …æ¬Šé‡ã€‚ç”¨é€”ï¼šè¨­ç½®L1æå¤±çš„æ¬Šé‡ã€‚
parser.add_argument("--per_weight", type=float, default=50.0, help="weight on per term for generator gradient")#50
# æ„ŸçŸ¥æå¤± for ç”Ÿæˆå™¨
parser.add_argument("--gan_weight", type=float, default=1.0, help="weight on GAN term for generator gradient")
parser.add_argument("--discrim_m", type=float, default=0.25, help="margin on GAN term for distrim percernal loss")
parser.add_argument("--dis_per_w", type=float, default=20.0, help="weight on GAN term for distrim percernal loss")#100
# parser.add_argument("--saveHide_freq", type=int, default=120000, help="ä¿å­˜éšè—å±‚")
# æ„ŸçŸ¥æå¤± for é‘‘åˆ¥å™¨
# --gan_weight:é¡å‹ï¼šfloaté»˜èªå€¼ï¼š1.0èªªæ˜ï¼šç”Ÿæˆå™¨æ¢¯åº¦çš„GANé …æ¬Šé‡ã€‚ç”¨é€”ï¼šè¨­ç½®GANæå¤±çš„æ¬Šé‡ã€‚
parser.add_argument("--cenSul_weight", type=float, default=50.0, help="weight on GAN term for central Sul loss")
parser.add_argument("--over_occlusion_weight", type=float, default=10.0, help="weight for over-occlusion loss")
parser.add_argument("--under_occlusion_weight", type=float, default=5.0, help="weight for under-occlusion loss")
# --cenSul_weight:é¡å‹ï¼šfloaté»˜èªå€¼ï¼š100.0èªªæ˜ï¼šä¸­å¤®æºæå¤±çš„æ¬Šé‡ã€‚ç”¨é€”ï¼šè¨­ç½®ä¸­å¤®æºæå¤±çš„æ¬Šé‡ã€‚
parser.add_argument("--collision_weight", type=float, default=100.0, help="weight for collision loss")
parser.add_argument("--output_filetype", default="png", choices=["png", "jpeg"])
parser.add_argument("--hist_weight", type=float, default=50.0, help="weight on GAN term for hist loss")
# parser.add_argument("--hist_weight", type=float, default=50.0, help="weight on GAN term for hist loss")


# --output_filetype:é¡å‹ï¼šstré»˜èªå€¼ï¼špngé¸é …ï¼špng, jpegèªªæ˜ï¼šè¼¸å‡ºæ–‡ä»¶é¡å‹ã€‚ç”¨é€”ï¼šè¨­ç½®è¼¸å‡ºæ–‡ä»¶çš„æ ¼å¼ã€‚

a = parser.parse_args()
# argparseæ¨¡å¡Šä¾†è§£æå‘½ä»¤è¡Œåƒæ•¸ã€‚æ‰€æœ‰åœ¨ä¹‹å‰å®šç¾©çš„åƒæ•¸éƒ½æœƒè¢«è§£æä¸¦å­˜å„²åœ¨aé€™å€‹è®Šé‡ä¸­ã€‚
EPS = 1e-12
CROP_SIZE = 256
# EPS: ä¸€å€‹éå¸¸å°çš„æ•¸ï¼Œç”¨ä¾†é¿å…æ•¸å€¼è¨ˆç®—ä¸­çš„é™¤é›¶éŒ¯èª¤ã€‚
# CROP_SIZE: å®šç¾©åœ–åƒè£å‰ªçš„å¤§å°ï¼Œé€™è£¡æ˜¯256ã€‚
Examples = collections.namedtuple("Examples", "paths, inputs, condition1, condition2, targets, count, steps_per_epoch")
# Examplesçš„å‘½åå…ƒçµ„ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
# paths: åœ–åƒè·¯å¾‘åˆ—è¡¨ã€‚
# inputs: è¼¸å…¥åœ–åƒã€‚
# condition1: æ¢ä»¶1ï¼ˆå¯èƒ½æ˜¯ç”¨æ–¼åœ–åƒè½‰æ›çš„æŸäº›æ¢ä»¶ï¼‰ã€‚
# condition2: æ¢ä»¶2ï¼ˆå¦ä¸€å€‹æ¢ä»¶ï¼‰ã€‚
# targets: ç›®æ¨™åœ–åƒï¼ˆçœŸå¯¦åœ–åƒæˆ–è½‰æ›å¾Œçš„åœ–åƒï¼‰ã€‚
# count: åœ–åƒçš„ç¸½æ•¸ã€‚
# steps_per_epoch: æ¯å€‹epochçš„æ­¥æ•¸ã€‚
# discrim_loss_perã€gen_per_loss
# Model = collections.namedtuple("Model",
#                                "outputs, predict_real, predict_fake, global_discrim_loss,local_discrim_loss,discrim_loss_per,discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1,gen_per_loss,gen_loss_CenSul, gen_grads_and_vars, train")
# Model = collections.namedtuple("Model",
#                                "outputs,predict_local_real0,predict_local_fake0, predict_local_real1,predict_local_fake1, predict_local_real2,predict_local_fake2, predict_real, predict_fake, global_discrim_loss,local_discrim_loss,discrim_loss_per,discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1,gen_per_loss,histogram_loss,gen_loss_CenSul, gen_grads_and_vars, train")

Model = collections.namedtuple("Model",
                               "outputs, predict_real, predict_fake, global_discrim_loss,local_discrim_loss,discrim_loss_per,discrim_grads_and_vars, gen_loss_GAN, gen_loss_L1,gen_per_loss,histogram_loss,gen_loss_CenSul,gen_loss_collision, gen_grads_and_vars, train")
# Modelçš„å‘½åå…ƒçµ„ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
# outputs: ç”Ÿæˆçš„åœ–åƒè¼¸å‡ºã€‚
# predict_real: å°çœŸå¯¦åœ–åƒçš„é æ¸¬çµæœã€‚
# predict_fake: å°å‡åœ–åƒçš„é æ¸¬çµæœã€‚
# global_discrim_loss: å…¨å±€åˆ¤åˆ¥å™¨çš„æå¤±ã€‚
# local_discrim_loss: å±€éƒ¨åˆ¤åˆ¥å™¨çš„æå¤±ã€‚
# discrim_grads_and_vars: åˆ¤åˆ¥å™¨çš„æ¢¯åº¦å’Œè®Šé‡ã€‚
# gen_loss_GAN: ç”Ÿæˆå™¨çš„GANæå¤±ã€‚
# gen_loss_L1: ç”Ÿæˆå™¨çš„L1æå¤±ã€‚
# gen_loss_CenSul: ç”Ÿæˆå™¨çš„ä¸­å¤®æºæå¤±ã€‚
# gen_grads_and_vars: ç”Ÿæˆå™¨çš„æ¢¯åº¦å’Œè®Šé‡ã€‚
# train: è¨“ç·´æ“ä½œã€‚

# GANä¸­ç”¨æ–¼åœ–åƒæ•¸æ“šçš„é è™•ç†
def preprocess(image):
    with tf.name_scope("preprocess"):
        # åœ–åƒåƒç´ å€¼å¾[0, 1]ç¯„åœè½‰æ›åˆ°[-1, 1]ç¯„åœã€‚
        # [0, 1] => [-1, 1]
        return image * 2 - 1

# GANä¸­ç”¨æ–¼åœ–åƒæ•¸æ“šçš„å¾Œè™•ç†
def deprocess(image):
    with tf.name_scope("deprocess"):
        # åœ–åƒåƒç´ å€¼å¾[-1, 1]ç¯„åœè½‰å›[0, 1]ç¯„åœã€‚
        # [-1, 1] => [0, 1]
        return (image + 1) / 2

# å°‡é¡è‰²åˆ†ç‚ºäº®åº¦ (L)ã€ç¶ ç´…åˆ†é‡ (a)ã€å’Œè—é»ƒåˆ†é‡ (b)
# preprocess lab
def preprocess_lab(lab):
    with tf.name_scope("preprocess_lab"):
        # åˆ†è§£æˆ Lã€a å’Œ b ä¸‰å€‹å–®ç¨çš„é€šé“ã€‚
        L_chan, a_chan, b_chan = tf.unstack(lab, axis=2)
        # L_chanï¼ˆäº®åº¦é€šé“ï¼‰ï¼šåŸå§‹ç¯„åœæ˜¯ [0, 100]ï¼Œè¢«è½‰æ›åˆ° [-1, 1] çš„ç¯„åœã€‚
        # è¨ˆç®—éç¨‹ï¼šL_chan / 50 - 1ï¼Œé€™æ¨£ [0, 100] æœƒå°æ‡‰åˆ° [-1, 1]ã€‚
        # a_chan å’Œ b_chanï¼ˆè‰²å½©é€šé“ï¼‰ï¼šåŸå§‹ç¯„åœå¤§ç´„æ˜¯ [-110, 110]ï¼Œä¹Ÿè¢«ç¸®æ”¾åˆ° [-1, 1]ã€‚
        # # è¨ˆç®—éç¨‹ï¼ša_chan / 110 å’Œ b_chan / 110
        # L_chan: black and white with input range [0, 100]
        # a_chan/b_chan: color channels with input range ~[-110, 110], not exact
        # [0, 100] => [-1, 1],  ~[-110, 110] => [-1, 1]
        return [L_chan / 50 - 1, a_chan / 110, b_chan / 110]

# æ¨¡å‹è¼¸å‡ºçš„æ¨™æº–åŒ–æ•¸æ“šè½‰å› Lab é¡è‰²ç©ºé–“ï¼Œæ¢å¾©åˆ°åŸå§‹çš„ç¯„åœï¼Œä»¥ä¾¿å¯è¦–åŒ–æˆ–ä½œç‚ºæœ€çµ‚è¼¸å‡ºã€‚
# L_chanï¼šé€†è½‰æ›å› [0, 100] çš„ç¯„åœã€‚
# è¨ˆç®—éç¨‹ï¼š(L_chan + 1) / 2 * 100ï¼Œé€™æ¨£ [-1, 1] æœƒè¢«è½‰æ›å› [0, 100]ã€‚
# a_chan å’Œ b_chanï¼šé€†è½‰æ›å›å¤§ç´„ [-110, 110] çš„ç¯„åœã€‚
# è¨ˆç®—éç¨‹ï¼ša_chan * 110 å’Œ b_chan * 110ã€‚
def deprocess_lab(L_chan, a_chan, b_chan):
    with tf.name_scope("deprocess_lab"):
        # this is axis=3 instead of axis=2 because we process individual images but deprocess batches
        #  æœ€å¾Œä½¿ç”¨ tf.stack å°‡ä¸‰å€‹é€šé“é‡æ–°åˆä½µæˆåœ–åƒ
        # é€™æ¬¡çš„ axis=3 æ˜¯å› ç‚ºè¦è™•ç†çš„æ˜¯ä¸€å€‹æ‰¹æ¬¡çš„åœ–åƒï¼Œé€šå¸¸å½¢ç‹€ç‚º [batch_size, height, width, 3]
        return tf.stack([(L_chan + 1) / 2 * 100, a_chan * 110, b_chan * 110], axis=3)


# L é€šé“ï¼šè¡¨ç¤ºäº®åº¦ï¼ˆlightnessï¼‰ï¼Œç¯„åœå¾ 0ï¼ˆé»‘è‰²ï¼‰åˆ° 100ï¼ˆç™½è‰²ï¼‰ã€‚
# a é€šé“ï¼šè¡¨ç¤ºå¾ç¶ è‰²åˆ°ç´…è‰²çš„è‰²å½©åˆ†ä½ˆï¼Œç¯„åœç´„ç‚º -110ï¼ˆç¶ è‰²ï¼‰åˆ° 110ï¼ˆç´…è‰²ï¼‰ã€‚
# b é€šé“ï¼šè¡¨ç¤ºå¾è—è‰²åˆ°é»ƒè‰²çš„è‰²å½©åˆ†ä½ˆï¼Œç¯„åœç´„ç‚º -110ï¼ˆè—è‰²ï¼‰åˆ° 110ï¼ˆé»ƒè‰²ï¼‰ã€‚
# åœ¨å°‡ç¶“éå¢å¼·çš„äº®åº¦é€šé“ (L channel) èˆ‡åœ–åƒçš„è‰²å½©é€šé“ (a å’Œ b channels) çµ„åˆï¼Œç„¶å¾Œå°‡å…¶è½‰æ›å› RGB é¡è‰²ç©ºé–“ã€‚
# ç›®çš„æ˜¯å°åœ–åƒé€²è¡Œå¢å¼·è™•ç†ï¼ˆå¦‚äº®åº¦èª¿æ•´ï¼‰ï¼Œä¸¦å°‡è™•ç†å¾Œçš„åœ–åƒè¼¸å‡ºç‚º RGB æ ¼å¼ã€‚
def augment(image, brightness):
    # (a, b) color channels, combine with L channel and convert to rgb
    # é€™è¡Œä»£ç¢¼å°‡ image æ²¿ç¬¬ 3 è»¸è§£å£“ï¼Œåˆ†è§£ç‚ºå–®ç¨çš„ a å’Œ b è‰²å½©é€šé“ã€‚
    a_chan, b_chan = tf.unstack(image, axis=3)
    # é€™è¡Œä»£ç¢¼å°‡ brightness æ²¿ç¬¬ 3 è»¸å£“ç¸®ï¼Œå»é™¤å–®é€šé“çš„ç¶­åº¦ï¼Œå¾—åˆ° L é€šé“ã€‚
    L_chan = tf.squeeze(brightness, axis=3)
    # ä½¿ç”¨ deprocess_lab å‡½æ•¸å°‡ Lã€a å’Œ b é€šé“é‡æ–°åˆä½µï¼Œä¸¦å°‡å…¶é€†è½‰æ›å›æ¨™æº–çš„ Lab è‰²å½©ç©ºé–“ç¯„åœã€‚
    lab = deprocess_lab(L_chan, a_chan, b_chan)
    # å°‡ Lab é¡è‰²ç©ºé–“çš„åœ–åƒè½‰æ›ç‚º RGB é¡è‰²ç©ºé–“ï¼Œä½¿å…¶é©åˆæ–¼å¯è¦–åŒ–æˆ–æ¨¡å‹è¼¸å‡ºã€‚
    rgb = lab_to_rgb(lab)
    return rgb


def conv(batch_input, out_channels, stride):
    with tf.variable_scope("conv"):
        # è¼¸å…¥å¼µé‡çš„æœ€å¾Œä¸€å€‹ç¶­åº¦ã€‚é€™ç”¨ä¾†å®šç¾©å·ç©æ ¸çš„è¼¸å…¥é€šé“æ•¸ã€‚
        in_channels = batch_input.get_shape()[3]
        # å»ºç«‹ä¸€å€‹å·ç©æ ¸ï¼Œå½¢ç‹€ç‚º [4, 4, in_channels, out_channels]ï¼Œ
        # å…¶ä¸­ 4x4 æ˜¯å·ç©æ ¸çš„ç©ºé–“å°ºå¯¸ï¼Œin_channels æ˜¯è¼¸å…¥é€šé“æ•¸ï¼Œout_channels æ˜¯è¼¸å‡ºçš„é€šé“æ•¸ã€‚
        # éš¨æ©Ÿåˆå§‹åŒ–å·ç©æ ¸çš„æ¬Šé‡ã€‚
        filter = tf.get_variable("filter", [4, 4, in_channels, out_channels], dtype=tf.float32,
                                 initializer=tf.random_normal_initializer(0, 0.02))
        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, in_channels, out_channels]
        #     => [batch, out_height, out_width, out_channels]
        # [0, 0]ï¼šå° batch ç¶­åº¦ä¸é€²è¡Œå¡«å……ã€‚
        # [1, 1]ï¼šå°é«˜åº¦ç¶­åº¦æ¯å´å¡«å…… 1ã€‚
        # [1, 1]ï¼šå°å¯¬åº¦ç¶­åº¦æ¯å´å¡«å…… 1ã€‚
        # [0, 0]ï¼šå°é€šé“ç¶­åº¦ä¸é€²è¡Œå¡«å……ã€‚
        # mode=CONSTANTæŒ‡å®šäº†å¡«å……çš„æ¨¡å¼
        padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode="CONSTANT")
        # [1, stride, stride, 1] è¡¨ç¤ºå·ç©é‹è¡Œçš„æ­¥é•·ï¼Œ
        # å…¶ä¸­ 1 è¡¨ç¤º batch å’Œ channels ç¶­åº¦ä¸é€²è¡Œæ­¥é€²ï¼Œåªåœ¨é«˜åº¦å’Œå¯¬åº¦ç¶­åº¦ä»¥ stride é€²è¡Œæ­¥é€²ã€‚
        # adding="VALID" è¡¨ç¤ºæ²’æœ‰é¡å¤–çš„å¡«å……ï¼Œé€™æ„å‘³è‘—è¼¸å‡ºå°ºå¯¸æœƒå› å·ç©æ ¸æ¸›å°ã€‚
        conv = tf.nn.conv2d(padded_input, filter, [1, stride, stride, 1], padding="VALID")
        return conv

# Leaky ReLUï¼ˆLeaky Rectified Linear Unitï¼‰æ¿€æ´»å‡½æ•¸
def lrelu(x, a):
    # ä¸¦ä½¿ç”¨ a ä½œç‚ºè² æ–œç‡ã€‚Leaky ReLU æ˜¯ ReLU çš„è®Šé«”ï¼Œ
    # åœ¨è¼¸å…¥å€¼ç‚ºè² æ™‚å…è¨±ä¸€äº›è² è¼¸å‡ºï¼Œé¿å… ReLU å‡ºç¾ã€Œæ­»äº¡ã€ç¥ç¶“å…ƒå•é¡Œã€‚
    # é€™æ¨£åœ¨è¼¸å…¥å€¼å°æ–¼é›¶æ™‚ï¼Œæ¿€æ´»å‡½æ•¸ä»æœƒæœ‰æ¢¯åº¦ï¼Œå¾è€Œèƒ½å¤ å¹«åŠ©ç¶²çµ¡å­¸ç¿’ã€‚
    with tf.name_scope("lrelu"):
        # adding these together creates the leak part and linear part
        # then cancels them out by subtracting/adding an absolute value term
        # leak: a*x/2 - a*abs(x)/2
        # linear: x/2 + abs(x)/2

        # this block looks like it has 2 inputs on the graph unless we do this
        # ç‚ºäº†ç¢ºä¿ x è¢«è¦–ç‚ºè¨ˆç®—åœ–ä¸­çš„ä¸€å€‹å–®ç¨ç¯€é»
        x = tf.identity(x)
        # 0.5 * (1 + a) * xï¼šé€™æ˜¯è¼¸å…¥å€¼ x çš„ç·šæ€§éƒ¨åˆ†ã€‚
        # 0.5 * (1 - a) * tf.abs(x)ï¼šé€™æ˜¯è¼¸å…¥å€¼çµ•å°å€¼çš„éƒ¨åˆ†ï¼Œç”¨æ–¼æ§åˆ¶ç•¶è¼¸å…¥ç‚ºè² æ™‚çš„è¼¸å‡ºã€‚
        # åœ¨æ­£æ•¸å€åŸŸï¼ŒLeaky ReLU å’Œ ReLU è¡¨ç¾ä¸€æ¨£ï¼Œåœ¨è² æ•¸å€åŸŸï¼Œè¼¸å‡ºæ˜¯ a * xï¼Œé€™å€‹ a æ˜¯è¨­ç½®çš„è² æ–œç‡ã€‚
        return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)

# æ‰¹æ­¸ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰æ“ä½œï¼Œå®ƒæœ‰åŠ©æ–¼åŠ é€Ÿè¨“ç·´é€Ÿåº¦å’Œæé«˜ç¥ç¶“ç¶²çµ¡çš„ç©©å®šæ€§ã€‚

def batchnorm(input):
    with tf.variable_scope("batchnorm"):
        #å‰µå»ºäº†ä¸€å€‹èˆ‡è¼¸å…¥ input ä¸€æ¨£çš„å¼µé‡ï¼Œä¸¦åœ¨è¨ˆç®—åœ–ä¸­å¢åŠ äº†ä¸€å€‹ç¯€é»
        input = tf.identity(input)
        # ç²å–è¼¸å…¥å¼µé‡ä¸­é€šé“çš„æ•¸é‡
        channels = input.get_shape()[3]
        # å°æ‡‰æ–¼æ‰¹æ­¸ä¸€åŒ–å¾Œçš„å¹³ç§»é …ï¼Œåˆå§‹åŒ–ç‚ºå…¨é›¶ã€‚
        offset = tf.get_variable("offset", [channels], dtype=tf.float32, initializer=tf.zeros_initializer())
        # å°æ‡‰æ–¼æ‰¹æ­¸ä¸€åŒ–å¾Œçš„ç¸®æ”¾é …ï¼Œåˆå§‹å€¼ç‚ºæœå¾å‡å€¼ç‚º 1.0ï¼Œæ¨™æº–å·®ç‚º 0.02 çš„æ­£æ…‹åˆ†ä½ˆã€‚
        scale = tf.get_variable("scale", [channels], dtype=tf.float32,
                                initializer=tf.random_normal_initializer(1.0, 0.02))
        # è¨ˆç®— input åœ¨ [0, 1, 2]ï¼ˆå³ batch, height, width ç¶­åº¦ï¼‰ä¸Šçš„å‡å€¼å’Œæ–¹å·®ï¼Œé€™æ¨£æ¯å€‹é€šé“æœƒå–®ç¨è¨ˆç®—å‡å€¼å’Œæ–¹å·®ã€‚
        mean, variance = tf.nn.moments(input, axes=[0, 1, 2], keep_dims=False)
        # æ˜¯ä¸€å€‹å°çš„å¸¸æ•¸ï¼Œç”¨æ–¼é˜²æ­¢åœ¨æ­¸ä¸€åŒ–æ™‚é™¤ä»¥é›¶ã€‚
        variance_epsilon = 1e-5
        # å°‡ input æ­£è¦åŒ–ï¼Œä½¿å…¶å…·æœ‰é›¶å‡å€¼å’Œå–®ä½æ–¹å·®ï¼Œç„¶å¾Œæ‡‰ç”¨ scale å’Œ offset
        # normalized = scale * ((input - mean) / sqrt(variance + variance_epsilon)) + offset
        normalized = tf.nn.batch_normalization(input, mean, variance, offset, scale, variance_epsilon=variance_epsilon)
        return normalized

# å¯¦ç¾åå·ç©
# batch_input:
# é€™æ˜¯è¼¸å…¥å¼µé‡ï¼Œå½¢ç‹€ç‚º [batch, in_height, in_width, in_channels]ã€‚
# ä»£è¡¨å¤šå€‹æ¨£æœ¬çš„æ‰¹æ¬¡ï¼ŒåŒ…å«é«˜åº¦å’Œå¯¬åº¦çš„ç‰¹å¾µåœ–ä»¥åŠé€šé“æ•¸ã€‚
# out_channels:
# è½‰ç½®å·ç©æ“ä½œçš„è¼¸å‡ºé€šé“æ•¸ï¼ˆç›®æ¨™ç‰¹å¾µåœ–çš„é€šé“æ•¸ï¼‰ã€‚
def deconv(batch_input, out_channels):
    with tf.variable_scope("deconv"):
        # è¼¸å…¥å¼µé‡ï¼Œå½¢ç‹€ç‚º [batch, in_height, in_width, in_channels]ã€‚
        batch, in_height, in_width, in_channels = [int(d) for d in batch_input.get_shape()]
        # tf.get_variable():ç”¨æ–¼å‰µå»ºæˆ–é‡ç”¨è®Šæ•¸ï¼Œç¢ºä¿å‘½åä¸€è‡´ï¼Œä¾¿æ–¼å…±äº«ã€‚
        # name="filter": å·ç©æ ¸çš„åç¨±ã€‚
        # [4, 4, out_channels, in_channels]: å·ç©æ ¸çš„å½¢ç‹€ï¼Œè§£é‡‹å¦‚ä¸‹ï¼š
        # 4, 4: å·ç©æ ¸çš„é«˜å’Œå¯¬ï¼ˆå³ 4x4ï¼‰ã€‚
        # out_channels: è¼¸å‡ºçš„é€šé“æ•¸ã€‚
        # in_channels: è¼¸å…¥çš„é€šé“æ•¸ã€‚
        # dtype=tf.float32: è³‡æ–™å‹åˆ¥ç‚º 32 ä½æµ®é»æ•¸ã€‚
        # initializer=tf.random_normal_initializer(0, 0.02):
        # ç”¨å‡å€¼ç‚º 0ã€æ¨™æº–å·®ç‚º 0.02 çš„æ­£æ…‹åˆ†ä½ˆåˆå§‹åŒ–å·ç©æ ¸æ¬Šé‡ã€‚
        filter = tf.get_variable("filter", [4, 4, out_channels, in_channels], dtype=tf.float32,
                                 initializer=tf.random_normal_initializer(0, 0.02))
        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, out_channels, in_channels]
        #     => [batch, out_height, out_width, out_channels]
        # tf.nn.conv2d_transpose:TensorFlow æä¾›çš„è½‰ç½®å·ç©å‡½æ•¸ã€‚å°‡ç‰¹å¾µåœ–å¾è¼ƒå°çš„åˆ†è¾¨ç‡ä¸Šæ¡æ¨£åˆ°è¼ƒå¤§çš„åˆ†è¾¨ç‡ã€‚
        # [batch, in_height * 2, in_width * 2, out_channels]:è¼¸å‡ºå½¢ç‹€ï¼Œç‰¹å¾µåœ–çš„é«˜åº¦å’Œå¯¬åº¦æ˜¯è¼¸å…¥çš„å…©å€ï¼Œé€šé“æ•¸æ˜¯ out_channelsã€‚
        # [1, 2, 2, 1]: æ­¥é•·ï¼ˆstrideï¼‰ï¼Œè¡¨ç¤ºåœ¨æ¯å€‹ç©ºé–“ç¶­åº¦ä¸Šæ­¥é•·ç‚º 2ï¼ˆå³ä¸Šæ¡æ¨£å€æ•¸ç‚º 2ï¼‰ï¼Œæ‰¹æ¬¡å’Œé€šé“ä¸è®Šï¼ˆæ­¥é•·ç‚º 1ï¼‰ã€‚
        # ä½¿ç”¨ SAME å¡«å……ï¼Œè¼¸å‡ºå¤§å°è¨ˆç®—ç‚ºï¼š out_size=in_sizeÃ—stride
        conv = tf.nn.conv2d_transpose(batch_input, filter, [batch, in_height * 2, in_width * 2, out_channels],
                                      [1, 2, 2, 1], padding="SAME")
        # è¿”å›è¨ˆç®—å¾Œçš„å¼µé‡ conv
        return conv


def check_image(image):
    # å–å¾—å¼µé‡ image çš„æœ€å¾Œä¸€ç¶­çš„å¤§å°ï¼ˆå³é¡è‰²é€šé“æ•¸ï¼‰ã€‚
    # ç¢ºä¿æœ€å¾Œä¸€ç¶­å¤§å°ç­‰æ–¼ 1ã€‚è‹¥ä¸æ»¿è¶³æ¢ä»¶ï¼Œå°‡è§¸ç™¼éŒ¯èª¤ä¸¦è¼¸å‡ºè‡ªè¨‚è¨Šæ¯
    assertion = tf.assert_equal(tf.shape(image)[-1], 1, message="image must have 1 color channels")
    # ç¢ºä¿åœ¨åŸ·è¡Œ tf.identity(image)ï¼ˆè¿”å› image æœ¬èº«ï¼‰ä¹‹å‰ï¼ŒåŸ·è¡Œ assertion çš„æª¢æŸ¥ã€‚
    # å¦‚æœæª¢æŸ¥å¤±æ•—ï¼Œç¨‹å¼æœƒå ±éŒ¯ä¸¦åœæ­¢åŸ·è¡Œ
    with tf.control_dependencies([assertion]):
        image = tf.identity(image)
    # image.get_shape().ndims:
    # ç²å–å¼µé‡ image çš„éœæ…‹ç¶­åº¦æ•¸ï¼ˆåœ¨ç·¨è­¯æ™‚å¯ç¢ºå®šçš„ç¶­åº¦æ•¸ï¼‰ã€‚
    # å¦‚æœ image çš„ç¶­åº¦æ•¸ä¸åœ¨ 3ï¼ˆå–®å¼µå½±åƒï¼Œå¦‚ [height, width, channels]ï¼‰
    # æˆ– 4ï¼ˆå½±åƒæ‰¹æ¬¡ï¼Œå¦‚ [batch_size, height, width, channels]ï¼‰ä¹‹é–“ï¼Œå°‡æ‹‹å‡ºéŒ¯èª¤ã€‚
    if image.get_shape().ndims not in (3, 4):
        raise ValueError("image must be either 3 or 4 dimensions")

    # image.get_shape() è¿”å›éœæ…‹å½¢ç‹€ï¼ˆå¦‚æœå¯èƒ½ï¼‰ã€‚å°‡å…¶è½‰æ›ç‚ºåˆ—è¡¨ä»¥é€²è¡Œä¿®æ”¹ã€‚
    # å°‡æœ€å¾Œä¸€ç¶­è¨­ç½®ç‚º 1 ä¸¦ä¿®æ”¹
    shape = list(image.get_shape())
    shape[-1] = 1
    image.set_shape(shape)
    return image

# RGB åˆ° Lab è‰²å½©ç©ºé–“
#  RGB è‰²å½©ç©ºé–“çš„å½±åƒè½‰æ›ç‚º CIELAB è‰²å½©ç©ºé–“ã€‚Lab æ˜¯ä¸€ç¨®æ„ŸçŸ¥å‡å‹»çš„è‰²å½©ç©ºé–“ï¼Œé©åˆç”¨æ–¼å½±åƒè™•ç†æˆ–æ©Ÿå™¨å­¸ç¿’æ‡‰ç”¨ã€‚
# based on https://github.com/torch/image/blob/9f65c30167b2048ecbe8b7befdc6b2d6d12baee9/generic/image.c
def rgb_to_lab(srgb):
    with tf.name_scope("rgb_to_lab"):
        srgb = check_image(srgb)
        # å°‡å½±åƒå±•å¹³æˆå½¢ç‹€ç‚º [num_pixels, 3] çš„å¼µé‡ï¼Œæ¯è¡Œä»£è¡¨ä¸€å€‹åƒç´ çš„ RGB å€¼ã€‚
        # tf.reshape æ ¹æ“šçµ¦å®šçš„å½¢ç‹€ï¼ŒæŒ‰ç…§æ•¸æ“šçš„å­˜å„²é †åºé‡æ–°çµ„ç¹”æ•¸æ“šã€‚
        srgb_pixels = tf.reshape(srgb, [-1, 3])
        # RGB åˆ° XYZ è½‰æ›ï¼Œå°‡ sRGB è½‰ç‚ºç·šæ€§ RGB
        with tf.name_scope("srgb_to_xyz"):
            # æ ¹æ“š sRGB çš„ç‰¹æ€§ï¼Œä½æ–¼ 0.04045 çš„å€¼éœ€ç”¨ä¸åŒå…¬å¼è½‰æ›ã€‚ tf.castæ”¹è®Šå¼µé‡æ•¸æ“šé¡å‹ 
            linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)
            exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)
            rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (
                        ((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask
            # å°‡éç·šæ€§çš„ sRGB å€¼è½‰ç‚ºç·šæ€§ RGB å€¼ã€‚
            rgb_to_xyz = tf.constant([
                #    X        Y          Z
                [0.412453, 0.212671, 0.019334],  # R
                [0.357580, 0.715160, 0.119193],  # G
                [0.180423, 0.072169, 0.950227],  # B
            ])
            xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)

        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions
        with tf.name_scope("xyz_to_cielab"):
            # convert to fx = f(X/Xn), fy = f(Y/Yn), fz = f(Z/Zn)
            # D65 ç™½é» æ˜¯ä¸€ç¨®å¸¸ç”¨çš„æ¨™æº–å…‰æºï¼Œæ¨¡æ“¬æ—¥å…‰çš„å…‰è­œåˆ†ä½ˆã€‚å°æ‡‰çš„æ¨™æº–ç™½é»å€¼æ˜¯ï¼š
            # Xn = 0.950456
            # Yn = 1.0
            # Zn = 1.088754
            # å°‡ XYZ æ­£è¦åŒ–åˆ° D65 ç™½é»ï¼š
            # tf.multiplyé€™æ˜¯ä¸€å€‹é€å…ƒç´ ä¹˜æ³•æ“ä½œ
            xyz_normalized_pixels = tf.multiply(xyz_pixels, [1 / 0.950456, 1.0, 1 / 1.088754])
            # å°‡æ­£è¦åŒ– XYZ å€¼æ˜ å°„åˆ° Lab çš„éç·šæ€§ç©ºé–“ï¼š
            epsilon = 6 / 29
            linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon ** 3), dtype=tf.float32)
            exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon ** 3), dtype=tf.float32)
            fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon ** 2) + 4 / 29) * linear_mask + (
                        xyz_normalized_pixels ** (1 / 3)) * exponential_mask
            # tf.constant çš„è¨­è¨ˆæ˜¯ç‚ºäº†èˆ‡ TensorFlow çš„è¨ˆç®—åœ–æ©Ÿåˆ¶å’Œæ•¸æ“šç®¡ç†å…¼å®¹
            # è€Œç›´æ¥ç”¨ Python çš„ array é›–ç„¶ç°¡å–®
            # ä½†åœ¨ TensorFlow æ“ä½œä¸­ä¸å¤ é«˜æ•ˆï¼Œä¹Ÿä¸æ–¹ä¾¿é€²è¡Œæ•¸æ“šé¡å‹ç®¡ç†æˆ–è·¨è¨­å‚™é‹ç®—
            # å°‡ fx, fy, fz æ˜ å°„åˆ° Labï¼š
            fxfyfz_to_lab = tf.constant([
                #  l       a       b
                [0.0, 500.0, 0.0],  # fx
                [116.0, -500.0, 200.0],  # fy
                [0.0, 0.0, -200.0],  # fz
            ])
            lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])
        # å°‡è½‰æ›å¾Œçš„åƒç´ é‡æ–° reshape ç‚ºè¼¸å…¥å½¢ç‹€ã€‚
        return tf.reshape(lab_pixels, tf.shape(srgb))
    
# CIE LAB è‰²å½©ç©ºé–“è½‰æ›ç‚º sRGB è‰²å½©ç©ºé–“
#  Lab åˆ° RGB è‰²å½©ç©ºé–“
# é€™æ®µç¨‹å¼å®Œæˆäº† LAB -> XYZ -> RGB -> sRGB çš„è½‰æ›ã€‚
# è©²éç¨‹åœ¨è‰²å½©è™•ç†é ˜åŸŸä¸­å¸¸ç”¨ï¼Œç‰¹åˆ¥æ˜¯ç”¨æ–¼åœ–åƒé¡¯ç¤ºå’Œç·¨è¼¯ã€‚
# LAB è‰²å½©ç©ºé–“æ¥è¿‘æ–¼äººé¡è¦–è¦ºçš„æ„ŸçŸ¥ï¼Œè½‰æ›ç‚º sRGB å¾Œå¯ä»¥ç”¨æ–¼è¢å¹•é¡¯ç¤ºã€‚
def lab_to_rgb(lab):
    with tf.name_scope("lab_to_rgb"):
        lab = check_image(lab)
        lab_pixels = tf.reshape(lab, [-1, 3])

        # https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions
        with tf.name_scope("cielab_to_xyz"):
            # å°‡ LAB è½‰æ›ç‚º XYZï¼ˆCIE XYZ è‰²å½©ç©ºé–“ï¼‰ï¼š
            # é€éçŸ©é™£ç›¸ä¹˜ä¾†å¯¦ç¾ã€‚
            lab_to_fxfyfz = tf.constant([
                #   fx      fy        fz
                [1 / 116.0, 1 / 116.0, 1 / 116.0],  # l
                [1 / 500.0, 0.0, 0.0],  # a
                [0.0, 0.0, -1 / 200.0],  # b
            ])
            fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)

            # convert to xyz
            # è½‰æ›ç‚ºå¯¦éš›çš„ XYZ è‰²å½©å€¼ï¼š
            #  ğ‘“(ğ‘‹)>ğœ–ï¼Œç”¨ ğ‘“(ğ‘‹)^3è¨ˆç®—ã€‚
            #  ğ‘“(ğ‘‹)=<ğœ–ï¼Œç”¨ ç·šæ€§è¨ˆç®—
            epsilon = 6 / 29
            linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)
            exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)
            xyz_pixels = (3 * epsilon ** 2 * (fxfyfz_pixels - 4 / 29)) * linear_mask + (
                        fxfyfz_pixels ** 3) * exponential_mask

            # denormalize for D65 white point
            # XYZ è‰²å½©å€¼é€²è¡Œ D65 ç™½é»çš„åæ­£è¦åŒ–
            xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])

        with tf.name_scope("xyz_to_srgb"):
            # ä½¿ç”¨æ¨™æº–çš„ XYZ-to-RGB çŸ©é™£å°‡ XYZ è‰²å½©ç©ºé–“è½‰æ›ç‚ºç·šæ€§ RGBã€‚
            xyz_to_rgb = tf.constant([
                #     r           g          b
                [3.2404542, -0.9692660, 0.0556434],  # x
                [-1.5371385, 1.8760108, -0.2040259],  # y
                [-0.4985314, 0.0415560, 1.0572252],  # z
            ])
            rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)
            # avoid a slightly negative number messing up the conversion
            # ä»»ä½•å°æ–¼clip_value_minçš„å€¼éƒ½è¢«è¨­å®šç‚ºclip_value_minã€‚ä»»ä½•å¤§æ–¼clip_value_maxçš„å€¼éƒ½æœƒè¨­å®šç‚ºclip_value_maxã€‚
            rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)
            # å°‡ç·šæ€§ RGB è½‰æ›ç‚ºæ¨™æº–çš„ sRGBï¼ˆä¼½ç‘ªæ ¡æ­£ï¼‰ï¼š
            # ç•¶ ğ‘…ğºğµâ‰¤0.0031308ï¼Œä½¿ç”¨ç·šæ€§å…¬å¼
            # ç•¶ ğ‘…ğºğµ>0.0031308ï¼Œä½¿ç”¨æŒ‡æ•¸å…¬å¼ã€‚
            linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)
            exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)
            srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + (
                        (rgb_pixels ** (1 / 2.4) * 1.055) - 0.055) * exponential_mask
        # å°‡æ‰å¹³åŒ–çš„ RGB çµæœé‡æ–°èª¿æ•´ç‚ºèˆ‡è¼¸å…¥åœ–åƒç›¸åŒçš„å½¢ç‹€ã€‚
        return tf.reshape(srgb_pixels, tf.shape(lab))


# load dataset
def load_examples():
    # å¾æŒ‡å®šç›®éŒ„ä¸­è®€å– .jpg æˆ– .png æ–‡ä»¶
    if a.input_dir is None or not os.path.exists(a.input_dir):
        raise Exception("input_dir does not exist")

    input_paths = glob.glob(os.path.join(a.input_dir, "*.jpg"))
    decode = tf.image.decode_jpeg
    if len(input_paths) == 0:
        input_paths = glob.glob(os.path.join(a.input_dir, "*.png"))
        decode = tf.image.decode_png

    if len(input_paths) == 0:
        raise Exception("input_dir contains no image files")

    def get_name(path):
        name, _ = os.path.splitext(os.path.basename(path))
        return name

    # å¦‚æœæ‰€æœ‰åœ–åƒæ–‡ä»¶åæ˜¯æ•¸å­—ï¼Œå‰‡æŒ‰æ•¸å€¼æ’åºï¼›å¦å‰‡æŒ‰å­—å…¸åºæ’åºã€‚
    if all(get_name(path).isdigit() for path in input_paths):
        input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))
    else:
        input_paths = sorted(input_paths)

    # load images
    with tf.name_scope("load_images"):
        # tf.train.string_input_producer å‰µå»ºæ–‡ä»¶è·¯å¾‘éšŠåˆ—ï¼Œä¸¦é€šé tf.WholeFileReader è®€å–æ–‡ä»¶ã€‚
        path_queue = tf.train.string_input_producer(input_paths, shuffle=a.mode == "train")
        reader = tf.WholeFileReader()
        paths, contents = reader.read(path_queue)
        raw_input = decode(contents)
        # å°‡åœ–åƒæ•¸æ“šè§£ç¢¼ç‚ºå¼µé‡ï¼Œä¸¦è½‰æ›ç‚º tf.float32ï¼ˆç¯„åœ [0,1][0,1]ï¼‰ã€‚
        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)
        #         ç¢ºä¿åœ–åƒæ˜¯å–®é€šé“ï¼ˆå¦‚ç°åº¦åœ–ï¼‰ï¼Œå¦å‰‡è§¸ç™¼éŒ¯èª¤ã€‚
        # ä½¿ç”¨ tf.set_shape æ˜ç¢ºè¨­å®šåœ–åƒçš„å½¢ç‹€ã€‚
        assertion = tf.assert_equal(tf.shape(raw_input)[2], 1, message="image does not have 1 channels")
        #print(tf.shape(raw_input)[2])
        with tf.control_dependencies([assertion]):
            raw_input = tf.identity(raw_input)

        raw_input.set_shape([None, None, 1])
        # å¦‚æœä½¿ç”¨ LAB è‰²å½©ç©ºé–“ï¼Œå°‡ RGB åœ–åƒè½‰æ›ç‚º LABï¼Œä¸¦æå–äº®åº¦å’Œé¡è‰²é€šé“ã€‚
        if a.lab_colorization:
            # load color and brightness from image, no B image exists here
            lab = rgb_to_lab(raw_input)
            L_chan, a_chan, b_chan = preprocess_lab(lab)
            a_images = tf.expand_dims(L_chan, axis=2)
            b_images = tf.stack([a_chan, b_chan], axis=2)
        else:
            # å¦å‰‡ï¼Œå°‡åœ–åƒåˆ†å‰²ç‚ºå¤šå€‹å€åŸŸï¼Œä¸¦æ¨™æº–åŒ–åˆ°ç¯„åœ [âˆ’1,1][âˆ’1,1]ã€‚åˆ†å‰²åœ–åƒ
            width = tf.shape(raw_input)[1]  # [height, width, channels]
            #a_images = preprocess(raw_input[:, :width // 4, :])
            #c_image1 = preprocess(raw_input[:, :width // 4, :])
            #c_image2 = preprocess(raw_input[:, :width // 4, :])
            #b_images = preprocess(raw_input[:, width // 4:, :])
            # å‡è¨­åœ–åƒæ°´å¹³æ‹¼æ¥äº† 4 å€‹å€åŸŸï¼ŒæŒ‰åˆ—ï¼ˆaxis=1ï¼‰åˆ†å‰²ã€‚
            # æ¯å€‹å€åŸŸé€²è¡Œæ¨™æº–åŒ–é è™•ç†ï¼ˆå¯èƒ½å°‡å€¼ç¸®æ”¾åˆ°ç¯„åœ [âˆ’1,1][âˆ’1,1]ï¼‰ã€‚
            a_imagesT, c_image1T, c_image2T, b_imagesT=tf.split(raw_input, 4, axis=1)
            a_images = preprocess(a_imagesT);
            c_image1 = preprocess(c_image1T);
            c_image2 = preprocess(c_image2T);
            b_images = preprocess(b_imagesT);
    # è¨­å®šæ±ºå®šè¼¸å…¥èˆ‡è¼¸å‡ºçš„æ–¹å‘ï¼š
    if a.which_direction == "AtoB":
        inputs, condit1, condit2, targets = [a_images, c_image1, c_image2,b_images]
    elif a.which_direction == "BtoA":
        inputs, condit1, condit2, targets = [b_images, c_image2, c_image1, a_images]
    else:
        raise Exception("invalid direction")

    # ç¢ºä¿è¼¸å…¥èˆ‡è¼¸å‡ºçš„éš¨æ©Ÿæ“ä½œï¼ˆå¦‚è£å‰ªã€ç¿»è½‰ç­‰ï¼‰ä¿æŒä¸€è‡´ã€‚
    seed = random.randint(0, 2 ** 31 - 1)

    def transform(image):
        r = image
        # éš¨æ©Ÿæ°´å¹³ç¿»è½‰ï¼šæ ¹æ“šè¨­å®šéš¨æ©Ÿç¿»è½‰åœ–åƒã€‚
        if a.flip:
            r = tf.image.random_flip_left_right(r, seed=seed)

        # area produces a nice downscaling, but does nearest neighbor for upscaling
        # assume we're going to be doing downscaling here
        # èª¿æ•´åœ–åƒå¤§å°ï¼šä½¿ç”¨å€åŸŸæ’å€¼æ³•å°‡åœ–åƒèª¿æ•´åˆ°æŒ‡å®šå¤§å°ã€‚
        r = tf.image.resize_images(r, [a.scale_size, a.scale_size], method=tf.image.ResizeMethod.AREA)
        # ä½¿ç”¨ tf.random.uniform åœ¨ç¯„åœ [0, a.scale_size - CROP_SIZE + 1) ç”Ÿæˆä¸€å€‹éš¨æ©Ÿåç§»é‡ã€‚
        offset = tf.cast(tf.floor(tf.random_uniform([2], 0, a.scale_size - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)
        # å¦‚æœ a.scale_size > CROP_SIZEï¼ŒåŸ·è¡Œè£å‰ªï¼Œå¾åœ–åƒä¸­éš¨æ©Ÿå–å‡ºä¸€å€‹å¤§å°ç‚º ğ¶ğ‘…ğ‘‚ğ‘ƒ_ğ‘†ğ¼ğ‘ğ¸Ã—ğ¶ğ‘…ğ‘‚ğ‘ƒ_ğ‘†ğ¼ğ‘ğ¸ çš„å€åŸŸã€‚
        if a.scale_size > CROP_SIZE:
            r = tf.image.crop_to_bounding_box(r, offset[0], offset[1], CROP_SIZE, CROP_SIZE)
        # å‰‡æ‹‹å‡ºç•°å¸¸ï¼Œå› ç‚ºåœ–åƒä¸æ‡‰å°æ–¼è£å‰ªå¤§å°
        elif a.scale_size < CROP_SIZE:
            raise Exception("scale size cannot be less than crop size")
        # è¿”å›ç¶“éç¿»è½‰ã€èª¿æ•´å¤§å°å’Œè£å‰ªçš„åœ–åƒã€‚
        return r

    # def transform(image):
    #     r = image
    #     # éš¨æ©Ÿæ°´å¹³ç¿»è½‰ï¼ˆå¯é¸ï¼‰
    #     if a.flip:
    #         r = tf.image.random_flip_left_right(r, seed=seed)

    #     # èª¿æ•´åœ–åƒå¤§å°è‡³ scale_size
    #     r = tf.image.resize_images(r, [a.scale_size, a.scale_size], method=tf.image.ResizeMethod.AREA)

    #     # è¨ˆç®—æ•´æ•¸çš„ offset_height å’Œ offset_width
    #     offset_height = int(95 * (a.scale_size / CROP_SIZE))
    #     offset_width = int(80 * (a.scale_size / CROP_SIZE))

    #     # è£å‰ªæ”¾å¤§å¾Œçš„å€åŸŸ
    #     r = tf.image.crop_to_bounding_box(
    #         r,
    #         offset_height=offset_height,
    #         offset_width=offset_width,
    #         target_height=CROP_SIZE,
    #         target_width=CROP_SIZE
    #     )

    #     return r

    # å°è¼¸å…¥ (inputs)ã€æ¢ä»¶åœ–åƒ (condit1 å’Œ condit2)ã€ç›®æ¨™åœ–åƒ (targets) åˆ†åˆ¥æ‡‰ç”¨ transform å‡½æ•¸é€²è¡Œé è™•ç†
    with tf.name_scope("input_images"):
        input_images = transform(inputs)

    with tf.name_scope("con_image1"):
        con_image1 = transform(condit1)

    with tf.name_scope("con_image2"):
        con_image2 = transform(condit2)

    with tf.name_scope("target_images"):
        target_images = transform(targets)
    # tf.train.batchï¼š
    # å°‡è™•ç†éçš„åœ–åƒå’Œå°æ‡‰çš„æ–‡ä»¶è·¯å¾‘æ‰“åŒ…æˆæ‰¹æ¬¡ã€‚
    paths_batch, inputs_batch, con1_batch, con2_batch, targets_batch\
        = tf.train.batch([paths, input_images, con_image1, con_image2, target_images],  batch_size=a.batch_size)
    #     steps_per_epochï¼šæ¯å€‹ epoch çš„è¨“ç·´æ­¥æ•¸ã€‚
    # len(input_paths)ï¼šç¸½åœ–åƒæ•¸é‡ã€‚
    # a.batch_sizeï¼šæ¯å€‹æ‰¹æ¬¡çš„åœ–åƒæ•¸é‡ã€‚
    # ä½¿ç”¨ math.ceil ç¢ºä¿å³ä½¿æ•¸æ“šé‡ä¸æ˜¯æ‰¹æ¬¡å¤§å°çš„æ•´æ•¸å€ï¼Œæœ€å¾Œä¸€å€‹ä¸å®Œæ•´çš„æ‰¹æ¬¡ä¹Ÿæœƒè¢«è¨ˆå…¥ã€‚
    
    steps_per_epoch = int(math.ceil(len(input_paths) / a.batch_size))
    # Examples å°è±¡ï¼š
    # ä¸€å€‹å°è£æ•¸æ“šé›†çš„çµæ§‹é«”æˆ–é¡ï¼ˆå‡è¨­å·²åœ¨ä»£ç¢¼å…¶ä»–éƒ¨åˆ†å®šç¾©ï¼‰ã€‚
    return Examples(
        paths=paths_batch,
        inputs=inputs_batch,
        condition1=con1_batch,
        condition2=con2_batch,
        targets=targets_batch,
        count=len(input_paths),
        steps_per_epoch=steps_per_epoch,
    )
def create_generator_groove(generator_inputs, generator_outputs_channels, condition=None):
    # layersï¼šå­˜å„²ç”Ÿæˆå™¨æ¨¡å‹çš„æ‰€æœ‰å±¤ï¼Œå¾ç·¨ç¢¼å™¨åˆ°è§£ç¢¼å™¨ã€‚
    layers = []
    # ç·¨ç¢¼å™¨é€šéé€å±¤ä¸‹æ¡æ¨£å°‡è¼¸å…¥çš„ç‰¹å¾µåœ–å°ºå¯¸æ¸›å°ï¼Œé€šé“æ•¸é€æ¼¸å¢åŠ ã€‚
    # ç¬¬ä¸€å±¤å–®ç¨å¯¦ç¾ï¼Œå¾ŒçºŒå±¤æ ¹æ“š layer_specs è‡ªå‹•ç”Ÿæˆã€‚
    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]
    if condition is not None:
        generator_inputs = tf.concat([generator_inputs, condition], axis=3)
    with tf.variable_scope("encoder_1"):
        output = conv(generator_inputs, a.ngf, stride=2)
        layers.append(output)
    # é€™äº›å±¤æœƒä¾æ¬¡å°è¼¸å…¥ç‰¹å¾µåœ–é€²è¡Œå¤šæ¬¡ä¸‹æ¡æ¨£ã€‚
    layer_specs = [
        a.ngf * 2,  # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]
        a.ngf * 4,  # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]
        a.ngf * 8,  # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]
        a.ngf * 8,  # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]
        a.ngf * 8,  # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]
        a.ngf * 8,  # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]
        a.ngf * 8,  # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]
    ]
    # æ¯å±¤éƒ½é€šé LeakyReLU æ¿€æ´»å‡½æ•¸é€²è¡Œè™•ç†ï¼Œä¸¦ä½¿ç”¨å·ç©å±¤å°‡ç‰¹å¾µåœ–é€²è¡Œä¸‹æ¡æ¨£ã€‚
    for out_channels in layer_specs:
        with tf.variable_scope("encoder_%d" % (len(layers) + 1)):
            rectified = lrelu(layers[-1], 0.2)
        # æ¯æ¬¡å·ç©å¾Œæ‡‰ç”¨æ‰¹é‡æ­£è¦åŒ–ï¼ˆbatch normalizationï¼‰ä¾†åŠ é€Ÿè¨“ç·´ä¸¦æé«˜ç©©å®šæ€§ã€‚
            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]
            convolved = conv(rectified, out_channels, stride=2)
            output = batchnorm(convolved)
        # å°‡æ¯ä¸€å±¤çš„è¼¸å‡ºæ·»åŠ åˆ° layers åˆ—è¡¨ä¸­ã€‚
            layers.append(output)
    # å®šç¾©è§£ç¢¼å™¨éƒ¨åˆ†çš„å±¤é…ç½®ï¼ŒåŒ…æ‹¬æ¯å±¤çš„è¼¸å‡ºé€šé“æ•¸ä»¥åŠä½¿ç”¨çš„ dropout æ¯”ä¾‹ï¼ˆ0.0 è¡¨ç¤ºç„¡ dropoutï¼‰ã€‚
    layer_specs = [
        (a.ngf * 8, 0.5),  # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]
        (a.ngf * 8, 0.5),  # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]
        (a.ngf * 8, 0.5),  # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]
        (a.ngf * 8, 0.0),  # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]
        (a.ngf * 4, 0.0),  # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]
        (a.ngf * 2, 0.0),  # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]
        (a.ngf, 0.0),  # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]
    ]
    # åœ¨æ¯å€‹è§£ç¢¼å™¨å±¤ä¸­ï¼Œæ ¹æ“šæ˜¯å¦éœ€è¦è·³èºé€£æ¥ï¼ˆskip connectionï¼‰ï¼Œå°‡ä¸Šä¸€å±¤çš„è¼¸å‡ºèˆ‡ç›¸æ‡‰ç·¨ç¢¼å™¨å±¤çš„è¼¸å‡ºé€²è¡Œæ‹¼æ¥ã€‚
    num_encoder_layers = len(layers)
    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):
        skip_layer = num_encoder_layers - decoder_layer - 1
        with tf.variable_scope("decoder_%d" % (skip_layer + 1)):
            if decoder_layer == 0:
                # first decoder layer doesn't have skip connections
                # since it is directly connected to the skip_layer
                input = layers[-1]
            else:
                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)

            rectified = tf.nn.relu(input)
            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]
            # ä½¿ç”¨åå·ç©å±¤ï¼ˆdeconvï¼‰é€²è¡Œä¸Šæ¡æ¨£ï¼Œå°‡åœ–åƒå°ºå¯¸æ“´å¤§ä¸¦æ¸›å°‘é€šé“æ•¸ã€‚
            output = deconv(rectified, out_channels)
            output = batchnorm(output)
            # æ ¹æ“šé…ç½®çš„ dropout æ¯”ä¾‹ï¼Œéš¨æ©Ÿä¸Ÿæ£„éƒ¨åˆ†å–®å…ƒä¾†é˜²æ­¢éæ“¬åˆã€‚
            if dropout > 0.0:
                output = tf.nn.dropout(output, keep_prob=1 - dropout)

            layers.append(output)
    # æœ€å¾Œä¸€å±¤è§£ç¢¼å™¨å°‡ä¾†è‡ªç·¨ç¢¼å™¨çš„ç¬¬ä¸€å±¤å’Œæœ€å¾Œä¸€å±¤çš„ç‰¹å¾µåœ–é€²è¡Œæ‹¼æ¥ï¼ˆè·³èºé€£æ¥ï¼‰ã€‚
    # ä½¿ç”¨åå·ç©å±¤é€²è¡Œä¸Šæ¡æ¨£ï¼Œå°‡åœ–åƒå°ºå¯¸æ“´å±•è‡³åŸä¾†çš„å¤§å°ï¼ˆ256x256ï¼‰ã€‚
    # æœ€å¾Œï¼Œä½¿ç”¨ tanh å‡½æ•¸ä¾†é€²è¡Œè¼¸å‡ºï¼Œé€™æ¨£å¯ä»¥å°‡è¼¸å‡ºçš„å€¼é™åˆ¶åœ¨ [-1, 1] ç¯„åœå…§ã€‚
    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]
    with tf.variable_scope("decoder_1"):
        input = tf.concat([layers[-1], layers[0]], axis=3)
        rectified = tf.nn.relu(input)
        output = deconv(rectified, generator_outputs_channels)
        output = tf.tanh(output)
        layers.append(output)
    # æœ€å¾Œè¿”å›ç”Ÿæˆå™¨çš„è¼¸å‡ºï¼Œå³ layers åˆ—è¡¨ä¸­çš„æœ€å¾Œä¸€å€‹å±¤ï¼Œé€™æ˜¯ç¶“éè§£ç¢¼å™¨è™•ç†å¾Œçš„æœ€çµ‚åœ–åƒã€‚
    return layers[-1]

def create_generator(generator_inputs, discrimCon1, discrimCon2, generator_outputs_channels):
    layers = []

    # encoder_1: [batch, 256, 256, in_channels] => [batch, 128, 128, ngf]
    # é€™è¡Œç¨‹å¼ç¢¼çš„ä½œç”¨æ˜¯å°‡ä¸‰å€‹å¼µé‡ï¼ˆgenerator_inputs, discrimCon1, discrimCon2ï¼‰æ²¿è‘—æœ€å¾Œä¸€å€‹ç¶­åº¦ï¼ˆé€šå¸¸æ˜¯é€šé“æ•¸ç¶­åº¦ï¼Œå³ axis=3ï¼‰é€²è¡Œæ‹¼æ¥ï¼ˆconcatenateï¼‰
    # ã€‚å…·é«”ä¾†èªªï¼Œé€™å€‹æ“ä½œçš„æ„åœ–æ˜¯å°‡ä¾†è‡ªä¸åŒä¾†æºçš„ç‰¹å¾µåœ–åˆä½µç‚ºä¸€å€‹å–®ä¸€çš„å¼µé‡ï¼Œ
    generator_inputss=tf.concat([generator_inputs, discrimCon1, discrimCon2], axis=3)
    with tf.variable_scope("encoder_1"):
        output = conv(generator_inputss, a.ngf, stride=2)
        layers.append(output)

    layer_specs = [
        a.ngf * 2,  # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]
        a.ngf * 4,  # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]
        a.ngf * 8,  # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]
        a.ngf * 8,  # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]
        a.ngf * 8,  # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]
        a.ngf * 8,  # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]
        a.ngf * 8,  # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]
    ]

    for out_channels in layer_specs:
        with tf.variable_scope("encoder_%d" % (len(layers) + 1)):
            rectified = lrelu(layers[-1], 0.2)
            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]
            convolved = conv(rectified, out_channels, stride=2)
            output = batchnorm(convolved)
            layers.append(output)

    layer_specs = [
        (a.ngf * 8, 0.5),  # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]
        (a.ngf * 8, 0.5),  # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]
        (a.ngf * 8, 0.5),  # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]
        (a.ngf * 8, 0.0),  # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]
        (a.ngf * 4, 0.0),  # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]
        (a.ngf * 2, 0.0),  # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]
        (a.ngf, 0.0),  # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]
    ]

    num_encoder_layers = len(layers)
    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):
        skip_layer = num_encoder_layers - decoder_layer - 1
        with tf.variable_scope("decoder_%d" % (skip_layer + 1)):
            if decoder_layer == 0:
                # first decoder layer doesn't have skip connections
                # since it is directly connected to the skip_layer
                input = layers[-1]
            else:
                input = tf.concat([layers[-1], layers[skip_layer]], axis=3)

            rectified = tf.nn.relu(input)
            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]
            output = deconv(rectified, out_channels)
            output = batchnorm(output)

            if dropout > 0.0:
                output = tf.nn.dropout(output, keep_prob=1 - dropout)

            layers.append(output)

    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, generator_outputs_channels]
    with tf.variable_scope("decoder_1"):
        input = tf.concat([layers[-1], layers[0]], axis=3)
        rectified = tf.nn.relu(input)
        output = deconv(rectified, generator_outputs_channels)
        output = tf.tanh(output)
        layers.append(output)

    return layers[-1]


# create model
def create_model(inputs, condition1, condition2, targets):
    def create_discriminator(discrim_inputs,discrim_con1, discrim_con2,  discrim_targets):
        n_layers = 4
        layers = []

        # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]
        # tf.concat åœ¨é€šé“ç¶­åº¦ï¼ˆaxis=3ï¼‰é€²è¡Œæ‹¼æ¥ï¼Œå°‡åˆ¤åˆ¥å™¨è¼¸å…¥ã€æ¢ä»¶å’Œç›®æ¨™åœ–åƒçµ„åˆåœ¨ä¸€èµ·ã€‚
        input = tf.concat([discrim_inputs, discrim_con1, discrim_con2, discrim_targets], axis=3)
        # input = tf.concat([discrim_inputs,discrim_targets], axis=3)
        # ç¬¬ä¸€å±¤ï¼ˆlayer_1ï¼‰ï¼šè¼¸å…¥å°ºå¯¸ç‚º [batch, 256, 256, in_channels * 2]ï¼Œ
        # ç¶“éå·ç©å¾Œå°ºå¯¸ç¸®å°ç‚º [batch, 128, 128, ndf]ï¼Œä¸¦ä½¿ç”¨ Leaky ReLU æ¿€æ´»å‡½æ•¸ï¼ˆlreluï¼‰ã€‚
        # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]
        with tf.variable_scope("layer_1"):
            convolved = conv(input, a.ndf, stride=2)
            rectified = lrelu(convolved, 0.2)
            # perlayers.append(rectified)
            layers.append(rectified)

        # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]
        # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]
        # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]
        # n_layersï¼šè¨­ç½®åˆ¤åˆ¥å™¨çš„å·ç©å±¤æ•¸é‡ï¼Œé€™è£¡å®šç¾©ç‚º 3 å±¤ã€‚
        # æ¥ä¸‹ä¾†æ˜¯ä¸‰å±¤å·ç©å±¤ï¼ˆlayer_2, layer_3, layer_4ï¼‰ï¼Œæ¯ä¸€å±¤çš„è¼¸å‡ºé€šé“æ•¸æœƒé€æ¼¸å¢å¤§ï¼ˆndf, ndf*2, ndf*4, ndf*8ï¼‰
        # ï¼Œä¸¦ä¸”æ¯å±¤çš„æ­¥å¹…ï¼ˆstrideï¼‰æœƒæ ¹æ“šéœ€è¦è¨­ç½®ç‚º 2ï¼ˆé™¤éæ˜¯æœ€å¾Œä¸€å±¤ï¼Œæ­¥å¹…ç‚º 1ï¼‰ã€‚
        for i in range(n_layers):
            with tf.variable_scope("layer_%d" % (len(layers) + 1)):
                out_channels = a.ndf * min(2 ** (i + 1), 8)
                stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1
                convolved = conv(layers[-1], out_channels, stride=stride)
                normalized = batchnorm(convolved)
                rectified = lrelu(normalized, 0.2)
                layers.append(rectified)
        # æœ€å¾Œä¸€å±¤ï¼ˆlayer_5ï¼‰å°‡å·ç©çš„è¼¸å‡ºæ˜ å°„åˆ° 1 é€šé“ï¼ˆå³äºŒåˆ†é¡çµæœï¼šçœŸæˆ–å‡ï¼‰
        # ä¸¦ä½¿ç”¨ Sigmoid å‡½æ•¸å°‡çµæœé™åˆ¶åœ¨ [0, 1] ç¯„åœå…§ï¼Œè¡¨ç¤ºåˆ¤åˆ¥çµæœã€‚
        # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]
        with tf.variable_scope("layer_%d" % (len(layers) + 1)):
            convolved = conv(rectified, out_channels=1, stride=1)
            output = tf.sigmoid(convolved)
            layers.append(output)
        # perlayersï¼šé€™å€‹åˆ—è¡¨åŒ…å«äº†æ¯ä¸€å±¤çš„è¼¸å‡ºçµæœã€‚å®ƒåŒ…æ‹¬æ‰€æœ‰ä¸­é–“å±¤ï¼ˆrectified æ¿€æ´»å¾Œçš„å±¤ï¼‰ï¼Œä»¥åŠæœ€çµ‚è¼¸å‡ºå±¤
        return layers
    # perceTargetï¼šç›®æ¨™åœ–åƒçš„ç‰¹å¾µè¡¨ç¤ºï¼Œé€šå¸¸æ˜¯å¾ä¸€å€‹é è¨“ç·´çš„ç¶²çµ¡ä¸­æå–çš„ç‰¹å¾µåœ–ã€‚
    # perceOutputï¼šç”Ÿæˆåœ–åƒçš„ç‰¹å¾µè¡¨ç¤ºï¼Œé€šå¸¸æ˜¯ç”Ÿæˆå™¨è¼¸å‡ºçš„åœ–åƒçš„ç‰¹å¾µåœ–ã€‚
    def perceptual_Loss(perceTarget, perceOutput):
        # è¨­å®šæ¯å±¤æ¬Šé‡
        weights = [1.0, 2.0, 2.0]
        perLoss = 0.0
        for i in range(len(perceTarget)-2):
            # Calculate the size of the feature map
            C, H, W = tf.shape(perceTarget[i])[1], tf.shape(perceTarget[i])[2], tf.shape(perceTarget[i])[3]
            normalization_factor = tf.cast(C * H * W, tf.float32)
            # Compute the mean absolute difference normalized by feature map size and weighted by lambda
            loss = weights[i] * tf.reduce_sum(tf.abs(perceTarget[i] - perceOutput[i])) / normalization_factor
            perLoss += loss
        return perLoss
    
    
    # gan local discriminator
    # ä¸»è¦é—œæ³¨åœ–åƒçš„ç‰¹å®šå€åŸŸï¼ˆROIï¼‰
    # discrim_inputs: åˆ¤åˆ¥å™¨çš„è¼¸å…¥ï¼Œé€šå¸¸æ˜¯ç”Ÿæˆå™¨ç”Ÿæˆçš„åœ–åƒã€‚
    # discrim_targets: åˆ¤åˆ¥å™¨çš„ç›®æ¨™ï¼Œé€šå¸¸æ˜¯çœŸå¯¦çš„åœ–åƒã€‚
    # èµ·å§‹é»ç‚º (80, 80)ï¼Œè¡¨ç¤ºå¾åœ–åƒçš„ç¬¬ 80 è¡Œã€ç¬¬ 80 åˆ—é–‹å§‹ã€‚
    # è£å‰ªçš„å¤§å°ç‚º (128, 128)ï¼Œè¡¨ç¤ºè£å‰ªå¾Œçš„å€åŸŸæ˜¯ 128x128 çš„æ­£æ–¹å½¢ã€‚
    def create_local_discriminator(discrim_inputs,discrim_con1, discrim_con2, discrim_targets):
        n_layers = 3
        layers = []

        #tensor ROIåŒºåŸŸè£å‰ª
        crop_inputs=tf.image.crop_to_bounding_box(discrim_inputs,80,80,128,128)
        crop_discrim_con1=tf.image.crop_to_bounding_box(discrim_con1,80,80,128,128)
        crop_discrim_con2 = tf.image.crop_to_bounding_box(discrim_con2, 80, 80, 128, 128)
        crop_targets = tf.image.crop_to_bounding_box(discrim_targets, 80, 80, 128, 128)

        # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]
        # å°‡è£å‰ªçš„è¼¸å…¥ï¼ˆcrop_inputsï¼‰å’Œç›®æ¨™ï¼ˆcrop_targetsï¼‰çµ„åˆç‚ºä¸€å€‹å¼µé‡ã€‚
        # å¦‚æœæ¯å€‹å¼µé‡çš„é€šé“æ•¸ç‚º Cï¼Œæ‹¼æ¥å¾Œçš„é€šé“æ•¸å°‡ç‚º 2Cã€‚   
        input = tf.concat([crop_inputs,crop_discrim_con1,crop_discrim_con2, crop_targets], axis=3)
        # è¼¸å…¥ï¼šæ‹¼æ¥å¾Œçš„å¼µé‡ã€‚
        # è¼¸å‡ºé€šé“æ•¸ï¼ša.nldfï¼ˆå±€éƒ¨åˆ¤åˆ¥å™¨çš„åŸºç¤é€šé“æ•¸ï¼‰ã€‚
        # æ­¥å¹…ï¼šstride=2ï¼Œè¡¨ç¤ºæ¯æ¬¡ç§»å‹•å…©å€‹åƒç´ ï¼Œç”¨æ–¼ä¸‹æ¡æ¨£ã€‚
        #  128x128x2C ä¸‹æ¡æ¨£ç‚º 64x64xC'ã€‚
        # layer_1: [batch, 128, 128, in_channels * 2] => [batch, 64, 64, nldf]
        with tf.variable_scope("layer_1"):
            convolved = conv(input, a.nldf, stride=2)
            rectified = lrelu(convolved, 0.2)
            layers.append(rectified)

        # layer_2: [batch, 64, 64, ndf ] => [batch, 32, 32, ndf * 2]
        # layer_3: [batch, 32, 32, ndf * 2] => [batch, 31, 31, ndf * 4]
        # ç–ŠåŠ åˆ¤åˆ¥å™¨çš„ä¸­é–“å·ç©å±¤ï¼Œæ¯å±¤åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
        # ç¬¬ä¸€å±¤è¼¸å‡ºé€šé“æ•¸ç‚º a.nldf * 2ã€‚
        # ç¬¬äºŒå±¤è¼¸å‡ºé€šé“æ•¸ç‚º a.nldf * 4ã€‚
        # å¦‚æœè¶…é 8 å€çš„åŸºç¤é€šé“æ•¸ï¼Œå°‡é€šé“æ•¸å›ºå®šç‚º a.nldf * 8ã€‚
        for i in range(n_layers):
            with tf.variable_scope("layer_%d" % (len(layers) + 1)):
                out_channels = a.nldf * min(2 ** (i + 1), 4)
                # ç•¶ i == n_layers - 1 æ™‚ï¼Œä½¿ç”¨ stride=1ï¼Œå¦å‰‡ä½¿ç”¨ stride=2ã€‚
                stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1
                convolved = conv(layers[-1], out_channels, stride=stride)
                normalized = batchnorm(convolved)
                rectified = lrelu(normalized, 0.2)
                layers.append(rectified)
        # æœ€å¾Œä¸€å±¤ä¸é€²è¡Œä¸‹æ¡æ¨£ï¼Œä»¥ä¿ç•™æ›´å¤šç‰¹å¾µä¿¡æ¯ã€‚
        # è¼¸å‡ºé€šé“æ•¸è¨­ç‚º 1ï¼Œå³æœ€å¾Œè¼¸å‡ºæ˜¯ä¸€å€‹å–®é€šé“çš„ç‰¹å¾µåœ–ã€‚
        # æ­¥å¹…è¨­ç‚º 1ï¼Œä¸é€²è¡Œä¸‹æ¡æ¨£ã€‚

        # layer_4: [batch, 31, 31, ndf * 4] => [batch, 30, 30, 1]
        with tf.variable_scope("layer_%d" % (len(layers) + 1)):
            convolved = conv(rectified, out_channels=1, stride=1)
            # sigmoid æ¿€æ´»å‡½æ•¸ï¼Œå°‡è¼¸å‡ºå€¼å£“ç¸®åˆ° [0, 1] å€é–“ï¼Œç”¨æ–¼åˆ¤åˆ¥å™¨çš„è¼¸å‡º
            output = tf.sigmoid(convolved)
            layers.append(output)
        # è¿”å› layers åˆ—è¡¨ä¸­çš„æœ€å¾Œä¸€å±¤ï¼ˆè¼¸å‡ºå¼µé‡ï¼‰ï¼Œè¡¨ç¤ºå±€éƒ¨åˆ¤åˆ¥å™¨çš„æœ€çµ‚åˆ¤å®šçµæœã€‚
        return layers

    # with tf.name_scope ç”¨ä¾†ç®¡ç†è¨ˆç®—åœ–ä¸­çš„å‘½åç©ºé–“ï¼Œå¾è€Œä½¿ä»£ç¢¼çš„çµæ§‹æ›´æ¸…æ™°ï¼Œä¾¿æ–¼èª¿è©¦å’ŒæŸ¥æ‰¾è®Šé‡ã€‚
    #  TensorBoard ä¸­å€åˆ†æˆ–æŸ¥çœ‹ä¸€çµ„ç›¸é—œæ“ä½œï¼ˆå¦‚æå¤±è¨ˆç®—ã€å¯è¦–åŒ–è¼¸å‡ºç­‰ï¼‰çš„æƒ…æ³ã€‚
    # with tf.variable_scope ç”¨æ–¼å°**è®Šé‡ï¼ˆvariablesï¼‰**é€²è¡Œå‘½åå’Œç®¡ç†ï¼Œç‰¹åˆ¥æ˜¯å…±äº«è®Šé‡çš„å ´æ™¯ã€‚ æ”¯æŒè®Šé‡é‡ç”¨
    # tf.variable_scope ç”¨æ–¼å…±äº«è®Šé‡ï¼ˆå¦‚ç”Ÿæˆå™¨å’Œåˆ¤åˆ¥å™¨çš„åƒæ•¸ï¼‰ï¼Œé¿å…å¤šæ¬¡å‰µå»ºç›¸åŒçš„è®Šé‡ã€‚
    # gan generator
    with tf.variable_scope("generator") as scope:
        out_channels = int(targets.get_shape()[-1])
        outputs = create_generator(inputs, condition1, condition2, out_channels)
    # å…¨å±€åˆ¤åˆ¥å™¨ï¼šå¾æ•´é«”åˆ¤æ–·çœŸå¯¦æ€§ï¼Œæ¯”å¦‚æ•´é«”è¼ªå»“å’Œçµæ§‹ã€‚
    # å±€éƒ¨åˆ¤åˆ¥å™¨ï¼šå°ˆæ³¨æ–¼ç´°ç¯€è™•ç†ï¼Œæ¯”å¦‚ç´‹ç†æˆ–å±€éƒ¨ä¸€è‡´æ€§ã€‚
    # create two copies of discriminator, one for real pairs and one for fake pairs
    # they share the same underlying variables
    with tf.name_scope("real_discriminator"):
        with tf.variable_scope("discriminator"):
            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]
            # ä¸€æ¬¡åˆ¤åˆ¥çœŸå¯¦åœ–åƒ (predict_real)ã€‚
            predict_real = create_discriminator(inputs, condition1, condition2, targets)
            
    with tf.name_scope("fake_discriminator"):
        with tf.variable_scope("discriminator", reuse=True):
            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]
            # ä¸€æ¬¡åˆ¤åˆ¥ç”Ÿæˆçš„å‡åœ–åƒ (predict_fake)ã€‚(é€šé reuse=True å…±äº«åƒæ•¸)ï¼Œ
            predict_fake = create_discriminator(inputs, condition1, condition2, outputs)

            # æå–ç‰¹å®šçš„ ROI (æ„Ÿèˆˆè¶£å€åŸŸï¼Œå¦‚ 128x128)ï¼Œé‡å°å°ç¯„åœåˆ¤æ–·çœŸå¯¦æ€§ã€‚   
    with tf.name_scope("real_local_discriminator"):
        with tf.variable_scope("local_discriminator"):
            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]
            predict_local_real = create_local_discriminator(inputs, condition1, condition2, targets)

    with tf.name_scope("fake_local_discriminator"):
        # Setting reuse=True avoids creating new variables and reuses the ones from the local  real discriminator.
        with tf.variable_scope("local_discriminator", reuse=True):
            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]
            predict_local_fake = create_local_discriminator(inputs, condition1, condition2, outputs)


    with tf.name_scope("discriminator_loss"):
        # minimizing -tf.log will try to get inputs to 1
        # predict_real => 1
        # predict_fake => 0
        # å¼•å…¥æ„ŸçŸ¥å±¤é¢çš„åˆ¤åˆ¥ã€‚
        discrim_loss_per = tf.nn.relu(tf.subtract(a.discrim_m,perceptual_Loss(predict_local_real,predict_local_fake)))
        # text{global_discrim_loss} = -/log(D(/text{real})) - /log(1 - D(/text{fake}))
        # å°çœŸå¯¦åœ–åƒè¼¸å‡ºå€¼é è¿‘ 1ï¼Œå°å‡åœ–åƒè¼¸å‡ºå€¼é è¿‘ 0ã€‚
        # åˆ¤åˆ¥å™¨æå¤±
        global_discrim_loss = tf.reduce_mean(predict_real[-1] + EPS) - tf.reduce_mean(predict_fake[-1] + EPS)
        # åˆ¤åˆ¥å±€éƒ¨ç‰¹å¾µæ˜¯å¦çœŸå¯¦ã€‚
        # local_discrim_loss=tf.reduce_mean(-(tf.log(predict_local_real[-1] + EPS) + tf.log(1 - predict_local_fake[-1] + EPS)))
        local_discrim_loss=tf.reduce_mean((predict_local_real[-1] + EPS) -tf.reduce_mean( predict_local_fake[-1] + EPS))

        discrim_loss =global_discrim_loss+local_discrim_loss+discrim_loss_per*a.dis_per_w
        
        # discrim_loss =global_discrim_loss+local_discrim_loss

    #flyadd æ„å»ºä¸­å¤®æ²Ÿæå–æ¨¡å‹
    with tf.name_scope("tarCentralSul_loss"):
        with tf.variable_scope("genTeethGroove"):
            cenSulTarget = create_generator_groove(targets,1)
    with tf.name_scope("outCentralSul_loss"):
        with tf.variable_scope("genTeethGroove", reuse=True):
            cenSulOutput = create_generator_groove(outputs,1)
    #flyadd


    with tf.name_scope("generator_loss"):
        # # å…ˆæŠŠé€™å¹¾å¼µåœ–éƒ½æ­¸ä¸€åˆ°åŒä¸€ç¯„åœ
        # # å…ˆæ­£è¦åŒ–åˆ° [0,1]
        # outputs_norm = (outputs + 1.) * 0.5
        # condition1_norm = (condition1 + 1.) * 0.5
        # targets_norm = (targets + 1.) * 0.5

        # # è¨ˆç®—ç°½åå·®ç•°
        # diff_gen_cond = outputs_norm - condition1_norm  # [batch, H, W, C]
        # diff_cond_tgt = condition1_norm - targets_norm  # [batch, H, W, C]

  
        # # ç¸½ç¢°æ’æå¤±ï¼šç”Ÿæˆåœ–åƒèˆ‡ç›®æ¨™åœ–åƒçš„ç¢°æ’ç‰¹å¾µå·®ç•°
        # gen_loss_collision = tf.reduce_mean(tf.abs(diff_gen_cond - diff_cond_tgt)) 
        # æ­£è¦åŒ–åˆ° [0, 1]
        outputs_norm = (outputs + 1.) * 0.5
        condition1_norm = (condition1 + 1.) * 0.5
        targets_norm = (targets + 1.) * 0.5

        # è¨ˆç®—å·®ç•°
        diff_gen_cond = outputs_norm - condition1_norm  # [batch, H, W, C]
        diff_cond_tgt = targets_norm - condition1_norm  # [batch, H, W, C]

        # ç‚ºæ­£è² åå·®è¨­ç½®ä¸åŒæ¬Šé‡
        positive_diff_gen = tf.nn.relu(diff_gen_cond)
        negative_diff_gen = tf.nn.relu(-diff_gen_cond)
        positive_diff_tgt = tf.nn.relu(diff_cond_tgt)
        negative_diff_tgt = tf.nn.relu(-diff_cond_tgt)

        # è¨ˆç®—åŠ æ¬Šæå¤±
        collision_loss = (a.over_occlusion_weight * tf.reduce_mean(positive_diff_gen) +
                        a.under_occlusion_weight * tf.reduce_mean(negative_diff_gen))
        target_collision_loss = (a.over_occlusion_weight * tf.reduce_mean(positive_diff_tgt) +
                                a.under_occlusion_weight * tf.reduce_mean(negative_diff_tgt))

        # ç¢°æ’æå¤±
        gen_loss_collision = tf.abs(collision_loss - target_collision_loss)
        # çœ‹èµ·ä¾†é€™é‚Šé‚„è¦åŠ å€¼æ–¹åœ–æå¤±å‡½å¼åœ¨L1é‚£é‚Š
        # GAN Loss=âˆ’log(D(G(z)))
        # -log(predict_fake)ï¼Œç•¶ predict_fake è¶¨è¿‘ 1 æ™‚ï¼Œæå¤±æœƒæ¥è¿‘ 0ã€‚
        gen_loss_GAN = tf.reduce_mean((predict_fake[-1]))#1
        #  (outputs) æ¥è¿‘ç›®æ¨™åœ–åƒ (targets)ï¼Œæé«˜ç”Ÿæˆçµæœçš„çœŸå¯¦æ€§ã€‚
        # L1 æå¤±å°åƒç´ å€¼å·®ç•°çš„æ•æ„Ÿæ€§è¼ƒä½ï¼Œé€šå¸¸æ¯” L2 æå¤±æ›´é©åˆåœ–åƒç”Ÿæˆã€‚
        gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))#2
        # # æ¯”è¼ƒç”Ÿæˆçš„åœ–åƒå’ŒçœŸå¯¦åœ–åƒåœ¨ç‰¹å¾µç©ºé–“ä¸­çš„å·®ç•° (ä¸æ˜¯åƒç´ å±¤é¢çš„ç›´æ¥å·®ç•°)ã€‚
        # # ç”¨æ³•ï¼šæé«˜ç”Ÿæˆåœ–åƒçš„é«˜å±¤æ¬¡æ„ŸçŸ¥ç›¸ä¼¼æ€§ï¼Œä¾‹å¦‚ç´‹ç†æˆ–å…§å®¹çš„ç›¸ä¼¼åº¦ã€‚
        # tf.nn.relu(tf.subtract(a.discrim_m,perceptual_Loss(predict_local_real,predict_local_fake)))
        gen_per_loss=perceptual_Loss(predict_local_real,predict_local_fake)#3
        # # ä½œç”¨ï¼šå°ˆæ³¨æ–¼ç”Ÿæˆå™¨å°ç›®æ¨™çš„ç‰¹å®šå€åŸŸ (å¦‚ä¸­å¤®æº) çš„ç”Ÿæˆè³ªé‡ï¼Œç¢ºä¿é€™éƒ¨åˆ†çš„æº–ç¢ºæ€§ã€‚
        gen_loss_CenSul=tf.reduce_mean(tf.abs(cenSulTarget - cenSulOutput))#4
        # ä½œç”¨ï¼šå°ç”Ÿæˆåœ–åƒç‰¹å¾µçš„åˆ†ä½ˆèˆ‡æ¢ä»¶ç‰¹å¾µ (condition2) é€²è¡Œæ¯”è¼ƒï¼Œç¢ºä¿ç”Ÿæˆçš„åœ–åƒç¬¦åˆæŒ‡å®šçš„æ¢ä»¶åˆ†ä½ˆã€‚
        hist_fake = tf.histogram_fixed_width(predict_fake[-1],  [0.0, 255.0], 256)
        hist_real = tf.histogram_fixed_width(predict_real[-1],  [0.0, 255.0], 256)

        histogram_loss = tf.reduce_mean(
        tf.divide(
            tf.square(tf.cast(hist_fake, tf.float32) - tf.cast(hist_real, tf.float32)),
            tf.maximum(1.0, tf.cast(hist_real, tf.float32))  # ç¢ºä¿é¡å‹ä¸€è‡´
        )
    )

        # å°‡ä¸Šè¿°å¤šå€‹æå¤±ä»¥æ¬Šé‡åŠ æ¬Šæ±‚å’Œï¼Œå¹³è¡¡ä¸åŒæå¤±çš„å½±éŸ¿ã€‚
        # gen_loss = gen_loss_GAN * a.gan_weight + gen_loss_L1 * a.l1_weight+gen_loss_CenSul*a.cenSul_weight+gen_per_loss * a.per_weight+histogram_loss * a.hist_weight
        # çµåˆæ‰€æœ‰æå¤±ï¼ŒåŠ æ¬Šæ±‚å’Œ
        gen_loss = (gen_loss_GAN * a.gan_weight +
                    gen_loss_L1 * a.l1_weight +
                    gen_loss_CenSul * a.cenSul_weight +
                    gen_per_loss * a.per_weight +
                    histogram_loss * a.hist_weight +
                    gen_loss_collision * a.collision_weight)

    # ä½œç”¨ï¼šä½¿ç”¨ Adam å„ªåŒ–å™¨æ›´æ–°èˆ‡åˆ¤åˆ¥å™¨ç›¸é—œçš„åƒæ•¸ï¼Œè®“å…¶å­¸ç¿’å¦‚ä½•æ›´å¥½åœ°å€åˆ†çœŸå¯¦èˆ‡ç”Ÿæˆåœ–åƒã€‚
    with tf.name_scope("discriminator_train"):
        discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith("discriminator")]
        discrim_optim = tf.train.AdamOptimizer(a.lr, a.beta1)
        discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)
        discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)
    # ä¿è­‰åœ¨æ¯æ¬¡ç”Ÿæˆå™¨æ›´æ–°å‰ï¼Œå…ˆå®Œæˆåˆ¤åˆ¥å™¨åƒæ•¸çš„æ›´æ–°ï¼Œå¯¦ç¾äº¤æ›¿è¨“ç·´ã€‚
    with tf.name_scope("generator_train"):
        with tf.control_dependencies([discrim_train]):
            # ä½¿ç”¨ Adam å„ªåŒ–å™¨æ›´æ–°èˆ‡ç”Ÿæˆå™¨ç›¸é—œçš„åƒæ•¸ï¼Œæå‡ç”Ÿæˆå™¨ç”Ÿæˆé«˜è³ªé‡åœ–åƒçš„èƒ½åŠ›ã€‚
            gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith("generator")]
            gen_optim = tf.train.AdamOptimizer(a.lr, a.beta1)
            gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)
            gen_train = gen_optim.apply_gradients(gen_grads_and_vars)


    # æŒ‡æ¨™çš„æ»‘å‹•å¹³å‡å€¼è¨ˆç®—ï¼Œç”¨æ–¼å¹³æ»‘æå¤±æ›²ç·šï¼Œè®“è¨“ç·´éç¨‹ä¸­çš„æŒ‡æ¨™æ›´åŠ ç©©å®š
    ema = tf.train.ExponentialMovingAverage(decay=0.99)
    #  gen_per_loss ,discrim_loss_per,
    # update_losses = ema.apply([global_discrim_loss,discrim_loss_per,local_discrim_loss, gen_loss_GAN, gen_loss_L1,gen_loss_CenSul, gen_per_loss,histogram_loss])
    update_losses = ema.apply([global_discrim_loss,discrim_loss_per,local_discrim_loss, gen_loss_GAN, gen_loss_L1,gen_loss_CenSul, gen_per_loss,histogram_loss,gen_loss_collision])
    # ç®¡ç†è¨“ç·´æ­¥é©Ÿï¼Œglobal_step æ˜¯ TensorFlow å…§å»ºè®Šé‡ï¼Œç”¨æ–¼è¨˜éŒ„ç•¶å‰è¨“ç·´é€²è¡Œçš„æ­¥æ•¸ã€‚
    global_step = tf.contrib.framework.get_or_create_global_step()
    # a.lr = tf.train.exponential_decay(0.0001, global_step, decay_steps=10000, decay_rate=0.96, staircase=True)
    incr_global_step = tf.assign(global_step, global_step + 1)
    # train=tf.group(update_losses, incr_global_step, gen_train) 
    # å°‡æŒ‡æ¨™æ›´æ–°ã€æ­¥é©Ÿéå¢å’Œç”Ÿæˆå™¨è¨“ç·´ç¶å®šåœ¨ä¸€èµ·ï¼Œå½¢æˆå®Œæ•´çš„è¨“ç·´æ­¥é©Ÿã€‚
    def mean_hide_layer(tensorlayer):
            tensordis=tensorlayer.get_shape().as_list()
            numdis=len(tensordis)
            lastdis=tensordis[numdis-1]
            layers=[]
            for i in range(lastdis):
                if i==0:
                    layers.append(tf.slice(tensorlayer, [0, 0, 0, i], [1, -1, -1, 1]))
                else:
                    templayer= layers[-1]+tf.slice(tensorlayer, [0, 0, 0, i], [1, -1, -1, 1])
                    layers.append(templayer)
            return layers[-1]/lastdis
    return Model(
        # predict_real=cenSulTarget,
        # predict_fake=cenSulOutput,
        # predict_local_real0=predict_local_real[0],
        # predict_local_fake0=predict_local_fake[0],
        # predict_local_real1=predict_local_real[1],
        # predict_local_fake1=predict_local_fake[1],
        # predict_local_real2=predict_local_real[2],
        # predict_local_fake2=predict_local_fake[2],
        predict_real=predict_real[-1],
        predict_fake=predict_fake[-1],
        global_discrim_loss=ema.average(global_discrim_loss),
        local_discrim_loss=ema.average(local_discrim_loss),
        discrim_loss_per=ema.average(discrim_loss_per),
        discrim_grads_and_vars=discrim_grads_and_vars,
        gen_loss_GAN=ema.average(gen_loss_GAN),
        gen_loss_L1=ema.average(gen_loss_L1),
        gen_loss_CenSul=ema.average(gen_loss_CenSul),
        gen_per_loss=ema.average(gen_per_loss),
        histogram_loss=ema.average(histogram_loss),
        gen_loss_collision=ema.average(gen_loss_collision),
        gen_grads_and_vars=gen_grads_and_vars,
        outputs=outputs,
        train=tf.group(update_losses, incr_global_step, gen_train),
    )

# å­˜å„²ã€Œè¼¸å…¥ (inputs)ã€ã€ã€Œè¼¸å‡º (outputs)ã€å’Œã€Œç›®æ¨™ (targets)ã€ã€‚
def save_images(fetches, step=None):
    image_dir = os.path.join(a.output_dir, "images")
    if not os.path.exists(image_dir):
        # å‰µå»ºä¿å­˜å½±åƒçš„è³‡æ–™å¤¾
        os.makedirs(image_dir)

    filesets = []
    # éæ­·æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰åœ–åƒï¼Œä¿å­˜è‡³ PNG æ–‡ä»¶
    for i, in_path in enumerate(fetches["paths"]):
        name, _ = os.path.splitext(os.path.basename(in_path.decode("utf8")))
        fileset = {"name": name, "step": step}
        for kind in ["inputs", "outputs", "targets"]:
            filename = name + "-" + kind + ".png"
            if step is not None:
                filename = "%08d-%s" % (step, filename)
            fileset[kind] = filename
            out_path = os.path.join(image_dir, filename)
            contents = fetches[kind][i]
            with open(out_path, "wb") as f:
                f.write(contents)
        # è¿”å›æ–‡ä»¶é›†åˆ—è¡¨ï¼Œè¨˜éŒ„æ¯å€‹ä¿å­˜æ–‡ä»¶çš„ç›¸é—œä¿¡æ¯ï¼š
        filesets.append(fileset)
    return filesets


def append_index(filesets, step=False):
    # ç”Ÿæˆæˆ–æ›´æ–°ä¸€å€‹ HTML æ–‡ä»¶ï¼Œç”¨æ–¼ä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºä¿å­˜çš„åœ–åƒï¼Œæ–¹ä¾¿ç›´è§€æŸ¥çœ‹è¼¸å…¥ã€è¼¸å‡ºèˆ‡ç›®æ¨™çš„æ¯”è¼ƒã€‚
    index_path = os.path.join(a.output_dir, "index.html")
    if os.path.exists(index_path):
        index = open(index_path, "a")
    else:
        index = open(index_path, "w")
        index.write("<html><body><table><tr>")
        if step:
            index.write("<th>step</th>")
        index.write("<th>name</th><th>input</th><th>output</th><th>target</th></tr>")

    for fileset in filesets:
        index.write("<tr>")

        if step:
            index.write("<td>%d</td>" % fileset["step"])
        index.write("<td>%s</td>" % fileset["name"])

        for kind in ["inputs", "outputs", "targets"]:
            index.write("<td><img src='images/%s'></td>" % fileset[kind])

        index.write("</tr>")
    return index_path



# a.cktCentralSulï¼šæŒ‡å®šä¸€å€‹è·¯å¾‘ï¼Œå¯èƒ½ç”¨æ–¼è®€å–ç‰¹å®šæ¨¡å‹æˆ–æ•¸æ“šæ–‡ä»¶ã€‚
# a.input_dirï¼šè¨“ç·´æ•¸æ“šçš„è¼¸å…¥ç›®éŒ„ã€‚
# a.mode = "train"ï¼šè¨­å®šç›®å‰é‹è¡Œæ¨¡å¼ç‚ºè¨“ç·´ã€‚
# a.output_dirï¼šè¨“ç·´è¼¸å‡ºçš„çµæœï¼ˆä¾‹å¦‚ï¼Œç”Ÿæˆçš„æ¨¡å‹æª”æ¡ˆï¼‰å°‡å­˜å„²åˆ°é€™å€‹ç›®éŒ„ã€‚
# a.max_epochsï¼šè¨­ç½®è¨“ç·´çš„æœ€å¤§ epoch æ•¸ï¼Œé€™è£¡æ˜¯ 400ã€‚
# a.which_directionï¼šè¨­å®šè½‰æ›æ–¹å‘ï¼ˆBtoA è¡¨ç¤ºå¾ B æ˜ å°„åˆ° Aï¼‰ã€‚
def main():
    #     if tf.__version__.split('.')[0] != "1":
    #         raise Exception("Tensorflow version 1 required")

    # # # # è®­ç»ƒçš„æ—¶å€™çš„å‚æ•°(ç”±äºé‡‡ç”¨
    a.cktCentralSul = "D:/Users/user/Desktop/weiyundontdelete/GANdata/trainingdepth/DAISdepth/alldata/DAISgroove/"

    # # # # # # # # # # è®­ç»ƒçš„æ—¶å€™çš„å‚æ•°(ç”±äºé‡‡ç”¨
    # # a.input_dir = 'D:/Users/user/Desktop/weiyundontdelete/GANdata/trainingdepth/DAISdepth/alldata/depthfordifferentr/DAISdepth/bb/r=2/final'
    # a.input_dir = 'D:/Users/user/Desktop/weiyundontdelete/GANdata/trainingdepth/DAISdepth/alldata/depthfordifferentr/DCPRdepth/bb/r=1/final'
    a.input_dir = "D:/Users/user/Desktop/weiyundontdelete/GANdata/forjournal/inlayonlay/train/final/"
    a.mode = "train"
    a.output_dir = "D:/Users/user/Desktop/weiyundontdelete/GANdata/forjournal/inlayonlay/train/trainmodel/"
    a.max_epochs=400
    a.which_direction = "BtoA"

    # a.checkpoint = "D:/Users/user/Desktop/weiyundontdelete/GANdata/trainingdepth/DAISdepth/alldata/model/DCPRdepth=1collisionandchangehistorgram/"
    # a.mode = "export"
    # a.output_dir ="D://Users//user//Desktop//weiyundontdelete//GANdata//trainingdepth//DAISdepth//alldata//exportmodel//DCPRdepth=1collisionandchangehistorgramr//"
    # a.which_direction = "BtoA"

    # æµ‹è¯•çš„æ—¶å€™çš„å‚æ•°
    #a.input_dir = "D:/Tensorflow/DAIS/test"
    #a.mode = "test"
    #a.output_dir = "D:/Tensorflow/DAIS/test_result"
    #a.checkpoint = "D:/Tensorflow/DAIS/Checkpoint"
    # ä¸‹é¢é€™å¥ç¨‹åºä¸ç”¨æ·»åŠ ï¼šå› çˆ²åœ¨checkpointä¸­å·²ç¶“æŠŠåŒ…å«äº† BtoAçš„option
    #  options = {"which_direction", "ngf", "ndf", "lab_colorization"}
    #     a.which_direction = "BtoA"

    #     python pix2pix.py /
    #   --mode test /
    #   --output_dir facades_test /
    #   --input_dir facades/val /
    #   --checkpoint facades_train

    #  ç‚ºéš¨æ©Ÿæ•¸ç”Ÿæˆå™¨è¨­ç½®ç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾ã€‚
    # å¦‚æœæœªæ‰‹å‹•æŒ‡å®šç¨®å­ï¼ˆa.seedï¼‰ï¼Œæœƒéš¨æ©Ÿç”Ÿæˆä¸€å€‹ç¨®å­ã€‚
    if a.seed is None:
        a.seed = random.randint(0, 2 ** 31 - 1)

    tf.set_random_seed(a.seed)
    np.random.seed(a.seed)
    random.seed(a.seed)

    # è¾“å‡ºç›®å½•è®¾ç½®
    if not os.path.exists(a.output_dir):
        os.makedirs(a.output_dir)

    if a.mode == "test" or a.mode == "export":
        if a.checkpoint is None:
            raise Exception("checkpoint required for test mode")
        # è®€å–æª¢æŸ¥é»æ–‡ä»¶ä¸­çš„ options.json é…ç½®ã€‚
        # options ä¸­çš„åƒæ•¸ï¼ˆä¾‹å¦‚ which_direction, ngf, ndf ç­‰ï¼‰æœƒæ ¹æ“šæª¢æŸ¥é»çš„å…§å®¹é€²è¡ŒåŠ è¼‰ã€‚
        # load some options from the checkpoint
        options = {"which_direction", "ngf", "ndf", "lab_colorization"}
        with open(os.path.join(a.checkpoint, "options.json")) as f:
            for key, val in json.loads(f.read()).items():
                if key in options:
                    print("loaded", key, "=", val)
                    setattr(a, key, val)
        # æ¸¬è©¦æ¨¡å¼ä¸‹ç¦ç”¨åœ–åƒç¿»è½‰å’Œç¸®æ”¾ç­‰æ•¸æ“šå¢å¼·åŠŸèƒ½ï¼Œç¢ºä¿æ¸¬è©¦çµæœä¸å—éš¨æ©Ÿæ€§å½±éŸ¿ã€‚
        a.scale_size = CROP_SIZE
        a.flip = False
    # åˆ—å‡ºç•¶å‰é‹è¡Œæ™‚çš„æ‰€æœ‰åƒæ•¸ï¼Œæ–¹ä¾¿æª¢æŸ¥è¨­ç½®æ˜¯å¦æ­£ç¢ºã€‚
    for k, v in a._get_kwargs():
        print(k, "=", v)
    # å°‡æ‰€æœ‰åƒæ•¸ä¿å­˜åˆ° options.json æ–‡ä»¶ï¼Œæ–¹ä¾¿æ—¥å¾Œæª¢æŸ¥æˆ–è¤‡ç¾è¨“ç·´éç¨‹ã€‚
    with open(os.path.join(a.output_dir, "options.json"), "w") as f:
        f.write(json.dumps(vars(a), sort_keys=True, indent=4))

    # æ¨¡å‹æµ‹è¯•
    # å°‡è¨“ç·´å¥½çš„ç”Ÿæˆå™¨æ¨¡å‹å°å‡ºç‚ºç¨ç«‹çš„ meta æ–‡ä»¶ï¼Œæ–¹ä¾¿å¾ŒçºŒç”¨æ–¼ç”Ÿæˆåœ–ç‰‡è€Œä¸ä¾è³´å®Œæ•´çš„è¨“ç·´ä»£ç¢¼ã€‚
    if a.mode == "export":
    # å¦‚æœ lab_colorization è¨­ç‚º Trueï¼Œå‰‡æ‹‹å‡ºç•°å¸¸ï¼Œå› ç‚ºè©²åŠŸèƒ½ä¸æ”¯æ´å°å‡ºã€‚
        if a.lab_colorization:
            raise Exception("export not supported for lab_colorization")

        # å®šç¾©è¼¸å…¥å ä½ç¬¦ï¼Œç”¨æ–¼æ¥æ”¶ Base64 æ ¼å¼çš„å­—ç¬¦ä¸²æ•¸æ“š
        input = tf.placeholder(tf.string, shape=[1])

        # è§£æ Base64 å­—ç¬¦ä¸²ç‚ºåŸå§‹å½±åƒæ•¸æ“š
        input_data = tf.decode_base64(input[0])

        # è§£ç¢¼ PNG å½±åƒæ•¸æ“š
        input_image = tf.image.decode_png(input_data)

        # å¦‚æœå½±åƒæœ‰ 4 å€‹é€šé“ (RGBA)ï¼Œå‰‡ç§»é™¤ Alpha é€šé“ (åƒ…ä¿ç•™ RGB)
        input_image = tf.cond(
            tf.equal(tf.shape(input_image)[2], 4),
            lambda: input_image[:, :, :1],
            lambda: input_image
        )

        # è½‰æ›å½±åƒæ•¸æ“šé¡å‹ç‚º float32ï¼Œä¸¦å°‡åƒç´ å€¼æ­¸ä¸€åŒ–åˆ° [0, 1]
        input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)

        # è¨­å®šå½±åƒå½¢ç‹€ï¼šæ‡‰ç‚º [CROP_SIZE, 3 * CROP_SIZE, 1]
        input_image.set_shape([CROP_SIZE, 3 * CROP_SIZE, 1])

        # å¢åŠ æ‰¹æ¬¡ç¶­åº¦ (batch dimension)ï¼Œè®Šç‚º [1, height, width, channels]
        batch_input = tf.expand_dims(input_image, axis=0)

        # ä½¿ç”¨ç”Ÿæˆå™¨è™•ç†å½±åƒ
        with tf.variable_scope("generator"):
            batch_output = deprocess(
                create_generator(
                    preprocess(batch_input[:, :, :256, :]),  # ç¬¬ä¸€å¼µ
                    preprocess(batch_input[:, :, 256:512, :]),  # ç¬¬äºŒå¼µ (æ¢ä»¶1)
                    preprocess(batch_input[:, :, 512:, :]),  # ç¬¬ä¸‰å¼µ (æ¢ä»¶2)
                    1
                )
            )

        # å°‡ç”Ÿæˆçš„å½±åƒæ•¸æ“šè½‰æ›ç‚º uint8 (0-255)
        output_image = tf.image.convert_image_dtype(batch_output, dtype=tf.uint8)[0]

        # æ ¹æ“šè¼¸å‡ºæ ¼å¼é¸æ“‡ç·¨ç¢¼æ–¹å¼ (PNG æˆ– JPEG)
        if a.output_filetype == "png":
            output_data = tf.image.encode_png(output_image)
        elif a.output_filetype == "jpeg":
            output_data = tf.image.encode_jpeg(output_image, quality=80)
        else:
            raise Exception("invalid filetype")

        # è½‰æ›ç‚º Base64 å­—ç¬¦ä¸²æ ¼å¼ï¼Œä»¥ä¾¿è¼¸å‡º
        output = tf.convert_to_tensor([tf.encode_base64(output_data)])

        # å®šç¾© key çš„å ä½ç¬¦ï¼Œèˆ‡è¼¸å…¥æ•¸æ“šä¸€èµ·ä½œç‚ºæ¨¡å‹çš„è¼¸å…¥
        key = tf.placeholder(tf.string, shape=[1])
        inputs = {
            "key": key.name,
            "input": input.name
        }
        tf.add_to_collection("inputs", json.dumps(inputs))

        # å®šç¾©è¼¸å‡ºçš„æ ¼å¼ï¼Œå°‡è™•ç†å¾Œçš„å½±åƒçµæœä½œç‚ºè¼¸å‡º
        outputs = {
            "key": tf.identity(key).name,
            "output": output.name,
        }
        tf.add_to_collection("outputs", json.dumps(outputs))

        # åˆå§‹åŒ–æ¨¡å‹è®Šé‡
        init_op = tf.global_variables_initializer()

        # è¨­å®š Saver ç‰©ä»¶ä»¥æ¢å¾©èˆ‡ä¿å­˜æ¨¡å‹æ¬Šé‡
        restore_saver = tf.train.Saver()
        export_saver = tf.train.Saver()

        with tf.Session() as sess:
            # åŸ·è¡Œè®Šé‡åˆå§‹åŒ–
            sess.run(init_op)

            print("loading model from checkpoint")
            # è¼‰å…¥æœ€è¿‘çš„ checkpoint
            checkpoint = tf.train.latest_checkpoint(a.checkpoint)
            restore_saver.restore(sess, checkpoint)

            print("exporting model")
            # å°‡æ¨¡å‹çš„è¨ˆç®—åœ– (meta graph) ä¿å­˜ç‚º .meta æ–‡ä»¶
            export_saver.export_meta_graph(filename=os.path.join(a.output_dir, "export.meta"))

            # ä¿å­˜æ¨¡å‹çš„æ¬Šé‡
            export_saver.save(sess, os.path.join(a.output_dir, "export"), write_meta_graph=False)

        return

    # ä»æ•°æ®é›†åŠ è½½æ ·æœ¬ï¼Œè¿”å›çš„æ•°æ®é€šå¸¸åŒ…å«è¾“å…¥ã€ç›®æ ‡å›¾åƒä»¥åŠç›¸å…³çš„è·¯å¾„ä¿¡æ¯ã€‚
    examples = load_examples()
    print("examples count = %d" % examples.count)
    # create_modelï¼šåˆ›å»ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¨¡å‹ã€‚
    # è¾“å…¥ï¼šexamples.inputs æ˜¯æ¨¡å‹çš„è¾“å…¥å›¾åƒï¼Œexamples.condition1/condition2 æ˜¯æ¡ä»¶ä¿¡æ¯ï¼Œexamples.targets æ˜¯ç›®æ ‡å›¾åƒã€‚
    # è¾“å‡ºï¼šåŒ…å«ç”Ÿæˆçš„å›¾åƒ (model.outputs)ã€çœŸå‡é¢„æµ‹ (model.predict_real å’Œ model.predict_fake) å’Œå„ç§æŸå¤±ã€‚
    model = create_model(examples.inputs, examples.condition1, examples.condition2, examples.targets)

    # å¦‚æœ lab_colorization ä¸º Trueï¼Œåˆ™è¾“å…¥å’Œç›®æ ‡å›¾åƒéœ€è¦ç‰¹æ®Šå¤„ç†ï¼š
    # AtoB æ¨¡å¼ï¼š
    # targets å’Œ outputs å¢å¼ºï¼šå°†è¾“å…¥çš„äº®åº¦ä¿¡æ¯åŠ åˆ°ç›®æ ‡å’Œè¾“å‡ºå›¾åƒä¸Šã€‚
    # inputs å»å¤„ç†ï¼šå°†è¾“å…¥å›¾åƒå»å¤„ç†ä¸ºå•é€šé“ç°åº¦å›¾åƒã€‚
    # BtoA æ¨¡å¼ï¼š
    # inputs å¢å¼ºï¼šå°†ç›®æ ‡å›¾åƒçš„äº®åº¦ä¿¡æ¯åŠ åˆ°è¾“å…¥å›¾åƒä¸Šã€‚
    # targets å’Œ outputs å»å¤„ç†ï¼šå»å¤„ç†ä¸ºæ­£å¸¸ RGB å›¾åƒã€‚
    if a.lab_colorization:
        if a.which_direction == "AtoB":
            # inputs is brightness, this will be handled fine as a grayscale image
            # need to augment targets and outputs with brightness
            targets = augment(examples.targets, examples.inputs)
            outputs = augment(model.outputs, examples.inputs)
            # inputs can be deprocessed normally and handled as if they are single channel
            # grayscale images
            inputs = deprocess(examples.inputs)
        elif a.which_direction == "BtoA":
            # inputs will be color channels only, get brightness from targets
            inputs = augment(examples.inputs, examples.targets)
            targets = deprocess(examples.targets)
            outputs = deprocess(model.outputs)
        else:
            raise Exception("invalid direction")
    # å¦‚æœ lab_colorization ä¸º Falseï¼Œåˆ™ç›´æ¥å¯¹è¾“å…¥ã€ç›®æ ‡å’Œç”Ÿæˆå›¾åƒåº”ç”¨å»å¤„ç†ï¼š
    # cenSulFake å’Œ cenSulRealï¼šæ¨¡å‹å¯¹äºçœŸå®å’Œç”Ÿæˆå›¾åƒçš„çœŸå‡é¢„æµ‹ç»“æœã€‚
    else:
        inputs = deprocess(examples.inputs)
        targets = deprocess(examples.targets)
        outputs = deprocess(model.outputs)
        cenSulFake = deprocess(model.predict_fake)
        cenSulReal = deprocess(model.predict_real)
    # å®½é«˜æ¯”è°ƒæ•´ï¼šå¦‚æœ a.aspect_ratio ä¸ä¸º 1ï¼Œåˆ™æŒ‰æ¯”ä¾‹è°ƒæ•´å›¾åƒçš„å®½é«˜ã€‚
    # ç±»å‹è½¬æ¢ï¼šå°†å›¾åƒæ•°æ®è½¬æ¢ä¸ºæ•´å‹ï¼ˆuint8ï¼‰ï¼Œä¾¿äºç¼–ç å’Œæ˜¾ç¤ºã€‚
    def convert(image):
        if a.aspect_ratio != 1.0:
            # upscale to correct aspect ratio
            size = [CROP_SIZE, int(round(CROP_SIZE * a.aspect_ratio))]
            image = tf.image.resize_images(image, size=size, method=tf.image.ResizeMethod.BICUBIC)

        return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)
    def save_images_hide(name,fetches):
        if not os.path.exists(name):
            os.makedirs(name)
        inputs_save = deprocess(fetches)
        shape_list=inputs_save.shape
        len_list=len(shape_list)
        num_batch=shape_list[len_list-1]
        #print(num_batch)
        #print(shape_list)
        for i in range(num_batch):
            image=inputs_save[0,:,:,i]
            cv2.imwrite(name+ str(i) + '.jpg', image)


        #im = Image.fromarray(image)
        #im.save('/home/yuanfly/pix2pix/facades/RestoreTeeth/img.jpg')
        #converted_saves = convert(inputs_save)

        #matplotlib.image.imsave('/home/yuanfly/pix2pix/facades/RestoreTeeth/img.png', inputs_save)
        #with open('/home/yuanfly/pix2pix/facades/RestoreTeeth/img.txt', "wb") as f:
            #f.write(inputs_save)
        return
    # reverse any processing on images so they can be written to disk or displayed to user
    with tf.name_scope("convert_inputs"):
        converted_inputs = convert(inputs)

    with tf.name_scope("convert_targets"):
        converted_targets = convert(targets)

    with tf.name_scope("convert_outputs"):
        converted_outputs = convert(outputs)

    with tf.name_scope("convert_cenSulFake"):
        converted_cenSulFake = convert(cenSulFake)

    with tf.name_scope("convert_cenSulReal"):
        converted_cenSulReal = convert(cenSulReal)
    # å¯¹åŠŸèƒ½ï¼šå°†è½¬æ¢åçš„å›¾åƒç¼–ç ä¸º PNG æ ¼å¼ï¼Œå¹¶å­˜å‚¨åˆ° display_fetches ä¸­ï¼Œä»¥ä¾¿åç»­ä¿å­˜æˆ–æ˜¾ç¤ºã€‚
    # è·¯å¾„ä¿¡æ¯ï¼šexamples.paths ä¿å­˜äº†ä¸æ¯ä¸ªå›¾åƒå¯¹åº”çš„æ–‡ä»¶è·¯å¾„ã€‚
    with tf.name_scope("encode_images"):
        display_fetches = {
            "paths": examples.paths,
            "inputs": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name="input_pngs"),
            "targets": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name="target_pngs"),
            "outputs": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name="output_pngs"),
        }
    # å°†è¾“å…¥ã€ç›®æ ‡ã€ç”Ÿæˆçš„å›¾åƒä»¥åŠçœŸå‡é¢„æµ‹çš„ç»“æœå†™å…¥ TensorBoardï¼Œä¾¿äºå¯è§†åŒ–æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚
    # summaries
    with tf.name_scope("inputs_summary"):
        tf.summary.image("inputs", converted_inputs)

    with tf.name_scope("targets_summary"):
        tf.summary.image("targets", converted_targets)

    with tf.name_scope("outputs_summary"):
        tf.summary.image("outputs", converted_outputs)

    with tf.name_scope("cenSulFake_summary"):
        tf.summary.image("sulFake", converted_cenSulFake)

    with tf.name_scope("cenSulReal_summary"):
        tf.summary.image("sulReal", converted_cenSulReal)
    #with tf.name_scope("predict_real_summary"):
        #tf.summary.image("predict_local_real0", tf.image.convert_image_dtype(model.predict_local_real0, dtype=tf.uint8))
        #tf.summary.image("predict_local_real1", tf.image.convert_image_dtype(model.predict_local_real1, dtype=tf.uint8))
        #tf.summary.image("predict_local_real2", tf.image.convert_image_dtype(model.predict_local_real2, dtype=tf.uint8))

    #with tf.name_scope("predict_fake_summary"):
        #tf.summary.image("predict_local_fake0", tf.image.convert_image_dtype(model.predict_local_fake0, dtype=tf.uint8))
        #tf.summary.image("predict_local_fake1", tf.image.convert_image_dtype(model.predict_local_fake1, dtype=tf.uint8))
        #tf.summary.image("predict_local_fake2", tf.image.convert_image_dtype(model.predict_local_fake2, dtype=tf.uint8))

    # with tf.name_scope("predict_real_summary"):
    #     tf.summary.image("predict_real", tf.image.convert_image_dtype(model.predict_real, dtype=tf.uint8))
    #
    # with tf.name_scope("predict_fake_summary"):
    #     tf.summary.image("predict_fake", tf.image.convert_image_dtype(model.predict_fake, dtype=tf.uint8))
    # è®°å½• GAN è®­ç»ƒä¸­çš„å„ç§æŸå¤±ï¼š
    # åˆ¤åˆ«å™¨æŸå¤±ï¼š
    # global_discriminator_lossï¼šå…¨å±€åˆ¤åˆ«å™¨æŸå¤±ã€‚
    # local_discriminator_lossï¼šå±€éƒ¨åˆ¤åˆ«å™¨æŸå¤±ã€‚
    # discriminator_loss_perï¼šä¸æ„ŸçŸ¥ç›¸å…³çš„åˆ¤åˆ«å™¨æŸå¤±ã€‚
    # ç”Ÿæˆå™¨æŸå¤±ï¼š
    # gen_loss_GANï¼šç”Ÿæˆå™¨å¯¹æŠ—æŸå¤±ã€‚
    # gen_loss_L1ï¼šç”Ÿæˆå™¨ L1 æŸå¤±ï¼ˆå›¾åƒé‡å»ºï¼‰ã€‚
    # gen_loss_CenSulï¼šç”Ÿæˆå™¨ä¸­å¿ƒæ²ŸæŸå¤±ï¼ˆç‰¹å®šä»»åŠ¡ç›¸å…³ï¼‰ã€‚
    # gen_per_lossï¼šæ„ŸçŸ¥æŸå¤±ã€‚
    # hist_lossï¼šç›´æ–¹å›¾æŸå¤±ï¼ˆè¡¡é‡åˆ†å¸ƒå·®å¼‚ï¼‰ã€‚
    tf.summary.scalar("generator_loss_collision", model.gen_loss_collision)
    tf.summary.scalar("global_discriminator_loss", model.global_discrim_loss)
    tf.summary.scalar("local_discriminator_loss", model.local_discrim_loss)
    tf.summary.scalar("discriminator_loss_per", model.discrim_loss_per)
    tf.summary.scalar("generator_loss_GAN", model.gen_loss_GAN)
    tf.summary.scalar("generator_loss_L1", model.gen_loss_L1)
    tf.summary.scalar("generator_loss_cenSul", model.gen_loss_CenSul)
    tf.summary.scalar("generator_loss_per", model.gen_per_loss)
    tf.summary.scalar("histogram_loss", model.histogram_loss)

    # å°æ‰€æœ‰å¯è¨“ç·´çš„è®Šæ•¸ï¼ˆå¦‚æ¬Šé‡ã€åç½®ï¼‰ç¹ªè£½ç›´æ–¹åœ–ï¼Œè¨˜éŒ„å…¶å€¼çš„åˆ†å¸ƒ
    for var in tf.trainable_variables():
        tf.summary.histogram(var.op.name + "/values", var)
    # è¨˜éŒ„ç”Ÿæˆå™¨èˆ‡åˆ¤åˆ¥å™¨ä¸­è®Šæ•¸çš„æ¢¯åº¦åˆ†å¸ƒã€‚
    for grad, var in model.discrim_grads_and_vars + model.gen_grads_and_vars:
        tf.summary.histogram(var.op.name + "/gradients", grad)
    # åŠŸèƒ½ï¼šè¨ˆç®—æ‰€æœ‰å¯è¨“ç·´åƒæ•¸çš„ç¸½æ•¸é‡ï¼Œé€šå¸¸ç”¨æ–¼ä¼°è¨ˆæ¨¡å‹çš„è¤‡é›œåº¦ã€‚
    # tf.reduce_prodï¼šè¨ˆç®—æ¯å€‹è®Šæ•¸çš„å…ƒç´ æ•¸é‡ï¼ˆå³å¼µé‡ç¶­åº¦çš„ç©ï¼‰ã€‚
    # tf.reduce_sumï¼šå°‡æ‰€æœ‰åƒæ•¸çš„æ•¸é‡åŠ ç¸½ã€‚
    with tf.name_scope("parameter_count"):
        parameter_count = tf.reduce_sum([tf.reduce_prod(tf.shape(v)) for v in tf.trainable_variables()])


    # saver = tf.train.Saver(max_to_keep=1)
    #fly modify
    #     åŠŸèƒ½ï¼šç²å– generator ç¯„ç–‡ä¸­çš„æ‰€æœ‰å¯è¨“ç·´è®Šæ•¸ï¼Œä¸¦ç‚ºå…¶å»ºç«‹ä¸€å€‹ Saver ç‰©ä»¶ã€‚
    # ç›®çš„ï¼šå¾æª¢æŸ¥é»ä¸­æ¢å¾©ç”Ÿæˆå™¨çš„æ¬Šé‡èˆ‡åç½®ï¼Œä½†é€™äº›åƒæ•¸ä¸åƒèˆ‡ä¹‹å¾Œçš„åå‘å‚³æ’­ã€‚
    #è·å–ç‰™é½¿ä¿®å¤æ¨¡å‹ï¼ˆç‰™é½¿æ²Ÿçªæå–ï¼‰ä¸­ç”Ÿæˆå™¨Gçš„Gçš„å·ç§¯æ ¸ï¼Œæ¥ä¸‹æ¥æ¢å¤å·ç§¯æ ¸çš„æƒé‡å’Œåç½® å¹¶ä¸”ä¸¤è€…ä¸å‚ä¸åå‘ä¼ æ’­ ï¼ˆwordåšå®¢ä¸Šæœ‰ä»‹ç»ï¼‰
    ref_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')
    saver = tf.train.Saver(ref_vars)
    # åŠŸèƒ½ï¼šåŒæ¨£é‡å° genTeethGroove ç¯„ç–‡çš„è®Šæ•¸é€²è¡Œä¿å­˜å’Œæ¢å¾©æ“ä½œã€‚
    # ç›®çš„ï¼šç”¨æ–¼å°ˆé–€è™•ç†ç‰™é½’æºæ§½çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ¬Šé‡èˆ‡åç½®è¢«ç¨ç«‹ç®¡ç†ã€‚
    cenSul_ref_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='genTeethGroove')
    cenSulsaver = tf.train.Saver(cenSul_ref_vars)
    # logdirï¼šæŒ‡å®šä¿å­˜æ—¥èªŒçš„ç›®éŒ„ï¼Œæ ¹æ“š trace_freq å’Œ summary_freq åˆ¤æ–·æ˜¯å¦å•Ÿç”¨ã€‚
    # Supervisorï¼šTensorFlow çš„é«˜ç´š APIï¼Œç”¨æ–¼ç®¡ç†æœƒè©±ã€æ—¥èªŒä¿å­˜ä»¥åŠæª¢æŸ¥é»ã€‚

    logdir = a.output_dir if (a.trace_freq > 0 or a.summary_freq > 0) else None
    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=0, saver=None)

    # åŠŸèƒ½ï¼šå•Ÿå‹•å— Supervisor ç®¡ç†çš„æœƒè©±ã€‚
    # åƒæ•¸è¨ˆç®—ï¼šåˆ—å°ç¸½åƒæ•¸æ•¸é‡ï¼Œä¾¿æ–¼æª¢æŸ¥æ¨¡å‹è¦æ¨¡
    with sv.managed_session() as sess:
        # print parameterâ€”â€”count
    # åŠŸèƒ½ï¼šå¾æŒ‡å®šè·¯å¾‘åŠ è¼‰æœ€æ–°çš„æª¢æŸ¥é»ï¼Œæ¢å¾©ç”Ÿæˆå™¨çš„æ¬Šé‡èˆ‡åç½®ã€‚
    # ç›®çš„ï¼šåœ¨æ¥çºŒè¨“ç·´æˆ–é€²è¡Œæ¸¬è©¦æ™‚ï¼Œé¿å…å¾é ­é–‹å§‹è¨“ç·´ã€‚
        print("parameter_count =", sess.run(parameter_count))
        if a.checkpoint is not None:
            print("loading teeth repair model from checkpoint")
            checkpoint = tf.train.latest_checkpoint(a.checkpoint)
            saver.restore(sess, checkpoint)
        # é‡å°ä¸­å¤®æºæ§½æ¨¡å‹ï¼Œå¾ a.cktCentralSul æŒ‡å®šçš„è·¯å¾‘åŠ è¼‰æ¬Šé‡ã€‚
        if a.cktCentralSul is not None:
            print("loading central groove model from checkpoint")
            ckpt = tf.train.get_checkpoint_state(a.cktCentralSul)
            cenSulsaver.restore(sess, ckpt.model_checkpoint_path)
        # fly modify
        # a.max_epochsï¼šè‹¥æŒ‡å®šæœ€å¤§è¨“ç·´ä¸–ä»£ï¼Œæ­¥æ•¸ç‚ºæ¯å€‹ä¸–ä»£çš„æ­¥æ•¸ä¹˜ä»¥ç¸½ä¸–ä»£æ•¸ã€‚
        # a.max_stepsï¼šç›´æ¥æŒ‡å®šç¸½æ­¥æ•¸ï¼Œè¦†è“‹ä¹‹å‰çš„è¨ˆç®—ã€‚
        max_steps = 2 ** 32
        if a.max_epochs is not None:
            max_steps = examples.steps_per_epoch * a.max_epochs
        if a.max_steps is not None:
            max_steps = a.max_steps
        # é™åˆ¶æ¸¬è©¦æ­¥æ•¸ç‚ºæ¸¬è©¦æ•¸æ“šçš„ç¸½æ­¥æ•¸èˆ‡è¨­å®šçš„æœ€å¤§æ­¥æ•¸ï¼ˆmax_stepsï¼‰ä¹‹é–“çš„æœ€å°å€¼ã€‚
        # åœ¨æ¯ä¸€æ­¥ï¼š
        # åŸ·è¡Œæ¸¬è©¦ï¼šé€šé sess.run åŸ·è¡Œæ¸¬è©¦éç¨‹ï¼Œç”Ÿæˆåœ–ç‰‡çµæœï¼ˆresultsï¼‰ã€‚
        # ä¿å­˜æ¸¬è©¦çµæœï¼šåˆ©ç”¨ save_images(results) ä¿å­˜è¼¸å‡ºçš„åœ–ç‰‡ã€‚
        # è¨˜éŒ„æ¸¬è©¦åœ–ç‰‡åç¨±ï¼šåœ¨çµ‚ç«¯è¼¸å‡ºæ¸¬è©¦åœ–ç‰‡çš„åç¨±ã€‚
        # æ›´æ–°ç´¢å¼•æ–‡ä»¶ï¼šé€šé append_index æ·»åŠ ç”Ÿæˆåœ–ç‰‡çš„ç´¢å¼•ï¼Œä¾¿æ–¼å¾ŒçºŒæª¢
        # æµ‹è¯•çš„å…¥å£
        if a.mode == "test":
            # testing
            # at most, process the test data once
            max_steps = min(examples.steps_per_epoch, max_steps)
            for step in range(max_steps):
                results = sess.run(display_fetches)
                filesets = save_images(results)
                for i, f in enumerate(filesets):
                    print("æµ‹è¯• image", f["name"])
                index_path = append_index(filesets)

            print("wrote index at", index_path)
        else:
            # training
            start = time.time()
            # è¨“ç·´å¾ªç’°é‹è¡Œè‡³ max_steps è¨­å®šçš„æœ€å¤§æ­¥æ•¸ã€‚
            # æ¯ä¸€æ­¥ï¼Œæ ¹æ“šä¸åŒé »ç‡ï¼ˆfreqï¼‰åŸ·è¡Œå°æ‡‰çš„æ“ä½œã€‚
            for step in range(max_steps):
            # ç”¨é€”ï¼šåˆ¤æ–·æ˜¯å¦éœ€è¦åŸ·è¡ŒæŸäº›æ“ä½œï¼ˆå¦‚ä¿å­˜æ¨¡å‹ã€è¨˜éŒ„æ‘˜è¦ç­‰ï¼‰ã€‚
            # æ¢ä»¶ï¼šç•¶é »ç‡å¤§æ–¼ 0 ä¸”ç•¶å‰æ­¥æ•¸ç¬¦åˆé »ç‡æ¢ä»¶ï¼Œæˆ–è€…æ˜¯æœ€å¾Œä¸€æ­¥ã€‚
                def should(freq):
                    return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)

                options = None
                run_metadata = tf.RunMetadata()
                #                 if should(a.trace_freq):
                #                     options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
                #                     run_metadata = tf.RunMetadata()
                # model.trainï¼šåŸ·è¡Œå–®æ¬¡è¨“ç·´æ­¥é©Ÿã€‚
                # sv.global_stepï¼šå–å¾—å…¨åŸŸæ­¥æ•¸ã€‚
                fetches = {
                    "train": model.train,
                    "global_step": sv.global_step,
                }
                # æ ¹æ“šé »ç‡æ·»åŠ æ“ä½œï¼š
                # è¨˜éŒ„æå¤±å€¼ï¼ˆå¦‚ global_discrim_loss, gen_loss_L1ï¼‰ã€‚
                # ä¿å­˜æ‘˜è¦ï¼ˆsummaryï¼‰ã€‚
                # ä¿å­˜é¡¯ç¤ºåœ–ç‰‡ï¼ˆdisplayï¼‰ã€‚
                # è¨˜éŒ„åŸ·è¡Œç—•è·¡ï¼ˆtraceï¼‰ã€‚

                if should(a.progress_freq):
                    fetches["global_discrim_loss"] = model.global_discrim_loss
                    fetches["local_discrim_loss"] = model.local_discrim_loss
                    fetches["discrim_loss_per"] = model.discrim_loss_per
                    fetches["gen_loss_GAN"] = model.gen_loss_GAN
                    fetches["gen_loss_L1"] = model.gen_loss_L1
                    fetches["gen_loss_CenSul"] = model.gen_loss_CenSul
                    fetches["gen_per_loss"] = model.gen_per_loss
                    fetches["histogram_loss"] = model.histogram_loss
                    fetches["gen_loss_collision"] = model.gen_loss_collision  # æ–°å¢

                # ç”¨ sess.run åŸ·è¡Œå®šç¾©çš„æ“ä½œï¼Œä¸¦è¿”å›çµæœ resultsã€‚
                if should(a.summary_freq):
                    fetches["summary"] = sv.summary_op
                #  ä¸åŒé »ç‡åŸ·è¡Œçš„æ“ä½œ
                if should(a.display_freq):
                    fetches["display"] = display_fetches
                # if should(a.saveHide_freq):
                #     fetches["hide_layer3"] =model.predict_local_fake2
                #     fetches["hide_layer2"] = model.predict_local_fake1
                #     fetches["hide_layer1"] = model.predict_local_fake0

                results = sess.run(fetches, options=options, run_metadata=run_metadata)
                # æ¯éš” summary_freq æ­¥è¨˜éŒ„ä¸€æ¬¡ TensorBoard æ‘˜è¦ï¼Œä¾¿æ–¼å¾ŒçºŒåˆ†æã€‚
                if should(a.summary_freq):
                    print("recording summary")
                    sv.summary_writer.add_summary(results["summary"], results["global_step"])
                # ä¿å­˜ç”Ÿæˆçš„é¡¯ç¤ºåœ–ç‰‡ï¼Œä¸¦æ›´æ–°ç´¢å¼•æ–‡ä»¶ã€‚
                if should(a.display_freq):
                    print("saving display images")
                    filesets = save_images(results["display"], step=results["global_step"])
                    append_index(filesets, step=True)
                # ä½¿ç”¨ RunMetadata è¨˜éŒ„å®Œæ•´çš„é‹è¡Œç—•è·¡ï¼Œä¾› TensorBoard åˆ†ææ¨¡å‹æ€§èƒ½ï¼ˆå¦‚é‹ç®—åœ–ã€å…§å­˜å ç”¨ï¼‰ã€‚
                if should(a.trace_freq):
                    print("recording trace")
                    sv.summary_writer.add_run_metadata(run_metadata, "step_%d" % results["global_step"])
                # é¡¯ç¤ºä¿¡æ¯ï¼š
                # ç•¶å‰è¨“ç·´çš„ä¸–ä»£ï¼ˆepochï¼‰èˆ‡æ­¥æ•¸ã€‚
                # è™•ç†é€Ÿç‡ï¼ˆåœ–ç‰‡/ç§’ï¼‰ã€‚
                # é è¨ˆå‰©é¤˜æ™‚é–“ï¼ˆåˆ†é˜ï¼‰ã€‚
                # åˆ—å°æå¤±å€¼ï¼šé¡¯ç¤ºç”Ÿæˆå™¨å’Œåˆ¤åˆ¥å™¨çš„å„ç¨®æå¤±ã€‚
                if should(a.progress_freq):
                    # global_step will have the correct step count if we resume from a checkpoint
                    train_epoch = math.ceil(results["global_step"] / examples.steps_per_epoch)
                    train_step = (results["global_step"] - 1) % examples.steps_per_epoch + 1
                    rate = (step + 1) * a.batch_size / (time.time() - start)
                    remaining = (max_steps - step) * a.batch_size / rate
                    print("progress  epoch %d  step %d  image/sec %0.1f  remaining %dm" % (
                    train_epoch, train_step, rate, remaining / 60))
                    print("global_discrim_loss", results["global_discrim_loss"])
                    print("local_discrim_loss", results["local_discrim_loss"])
                    print("discrim_loss_per", results["discrim_loss_per"])
                    print("gen_loss_GAN", results["gen_loss_GAN"])
                    print("gen_loss_L1", results["gen_loss_L1"])
                    print("gen_loss_CenSul", results["gen_loss_CenSul"])
                    print("gen_per_loss", results["gen_per_loss"])
                    print("histogram_loss", results["histogram_loss"])
                    print("gen_loss_collision", results["gen_loss_collision"])  # æ–°å¢
                    
                    

                # æ¯éš” save_freq æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹çš„æª¢æŸ¥é»ã€‚
                if should(a.save_freq):
                    print("saving model")
                    saver.save(sess, os.path.join(a.output_dir, "model"), global_step=sv.global_step)
                # ä½¿ç”¨ Supervisor åˆ¤æ–·æ˜¯å¦éœ€è¦ä¸­æ–·è¨“ç·´ã€‚
                # if should(a.saveHide_freq):
                #     print("ä¿å­˜ä¸­é—´å±‚å›¾åƒ")
                #     layer='D://Users//user//Desktop//weiyundontdelete//GANdata//trainingdepth//DAISdepth//alldata//layer//'
                #     save_images_hide(layer+str(3)+'/'+str(results["global_step"])+'/',results["hide_layer3"])
                #     save_images_hide(layer+str(2)+'/'+ str(results["global_step"])+'/',results["hide_layer2"])
                #     save_images_hide(layer+str(1)+'/'+ str(results["global_step"])+'/',results["hide_layer1"])
                
                if sv.should_stop():
                    break


main()
